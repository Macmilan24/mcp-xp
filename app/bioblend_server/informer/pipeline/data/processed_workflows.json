[
  {
    "workflow_id": "iwc_assembly-hifi-hic-phasing-vgp4",
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-hic-phasing-vgp4",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
      "Cut1",
      "__UNZIP_COLLECTION__",
      "Convert characters1",
      "param_value_from_file",
      "__EXTRACT_DATASET__",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
      "join1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/imagemagick_image_montage/imagemagick_image_montage/7.1.2-2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_uniq_tool/9.5+galaxy2",
      "Show beginning1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
    ],
    "readme_cleaned": "Genome Assembly from Hifi reads with HiC phasing - VGP4 Generate phased assembly based on PacBio Hifi Reads using HiC data from the same individual for assembly phasing. Part of the VGP workflow suite, it needs to be run after the k-mer profiling workflow VGP1. Inputs 1. Hifi long reads [fastq]. 2. Trim Hi-C reads ? If yes, trim 5 bases at the beginning of each reads. Use with some Arima Hi-C data if the contact map looks \"noisy\". 3. Paired collection of Hi-C reads [fastq]. 4. Genome profile summary generated by Genomescope [txt] generated by VGP1 workflow. 5. K-mer database [meryldb] generated by VGP1 workflow. 6. Database to use for Busco lineages. Recommended : latest version. 7. Lineage. Select the taxonomic lineage of the assembled species. 8. Name of first assembly. 9. Name of second assembly. 10. Bits for bloom filter. Change for large genomes to save memory. 11. Homozygous Read Coverage. Optional: specify if the coverage detected by Genomescope in VGP1 in not satisfactory. 12. Genomescope model parameters [tabular] generated by VGP1 workflow. Outputs 1. Haplotype 1 assembly ([fasta] and [gfa]) 2. Haplotype 2 assembly ([fasta] and [gfa]) 3. Trimmed Hi-C reads collection 4. QC: MultiQC report for HiFi reads trimming 5. QC: Compleasm report for both assemblies 6. QC: Merqury report for both assemblies 7. QC: Assembly statistics for both assemblies 8. QC: Nx plot for both assemblies 9. QC: Size plot for both assemblies",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Assembly-Hifi-HiC-phasing-VGP4/Assembly-Hifi-HiC-phasing-VGP4.ga"
  },
  {
    "workflow_id": "iwc_assembly-hifi-trio-phasing-vgp5",
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-trio-phasing-vgp5",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
      "param_value_from_file",
      "Convert characters1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "__EXTRACT_DATASET__",
      "join1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
    ],
    "readme_cleaned": "Genome Assembly with Hifi reads and Trio Data Generate phased assembly based on PacBio Hifi Reads using parental Illumina data for phasing. Part of the VGP workflow suite, it needs to be run after the Trio k-mer Profiling workflow VGP2. Inputs 1. Hifi long reads [fastq] 2. Concatenated Illumina reads : Paternal [fastq] 3. Concatenated Illumina reads : Maternal [fastq] 4. K-mer database [meryldb] generated by VGP2 workflow. 5. Paternal hapmer database [meryldb] generated by VGP2 workflow. 6. Maternal hapmer database [meryldb] generated by VGP2 workflow. 7. Bits for Bloom Filter. Change for large genomes to save memory. 8. Database to use for Busco lineages. Recommended : latest version. 8. Lineage. Select the taxonomic lineage of the assembled species. 9. Homozygous read coverage (Estimated from the Genomescope model if not provided) 10. Genome model parameters generated by Genomescope [tabular] generated by VGP2 workflow. 11. Genome profile summary generated by Genomescope [txt] generated by VGP2 workflow. 12. Name of first haplotype 13. Name of second haplotype Outputs 1. Haplotype 1 assembly [fasta] and [gfa] 2. Haplotype 2 assembly [fasta] and [gfa] 9. QC: Size plot for both assemblies 5. QC: MultiQC report for HiFi reads trimming 6. QC: BUSCO report for both assemblies 7. QC: Compleasm report for both assemblies 8. QC: Merqury report for both assemblies 9. QC: Assembly statistics for both assemblies 10. QC: Nx plot for both assemblies 11. QC: Size plot for both assemblies",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Assembly-Hifi-Trio-phasing-VGP5/Assembly-Hifi-Trio-phasing-VGP5.ga"
  },
  {
    "workflow_id": "iwc_assembly-hifi-only-vgp3",
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-hifi-only-vgp3",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy2",
      "Cut1",
      "Convert characters1",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/bgruening/hifiasm/hifiasm/0.25.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "__EXTRACT_DATASET__",
      "join1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
    ],
    "readme_cleaned": "Contiging Solo: Generate assembly based on PacBio Hifi Reads. Inputs 1. Hifi long reads [fastq] 2. K-mer database [meryldb] 3. Genome profile summary generated by Genomescope [txt] 4. Homozygous Read Coverage. Optional, use if you think the estimation from Genomescope is inacurate. 5. Genomescope Model Parameters generated by Genomescope [tabular] 6. Database for busco lineage (recommended: latest) 7. Busco lineage (recommended: vertebrata) 8. Name of first assembly 9. Name of second assembly Outputs 1. Primary assembly 2. Alternate assembly 3. QC: Bandage image for the raw unitigs 4. QC: BUSCO report for both assemblies 5. QC: Compleasm report for both assemblies 6. QC: Merqury report for both assemblies 7. QC: Assembly statistics for both assemblies 8. QC: Nx plot for both assemblies 9. QC: Size plot for both assemblie",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Assembly-Hifi-only-VGP3/Assembly-Hifi-only-VGP3.ga"
  },
  {
    "workflow_id": "iwc_assembly-decontamination-vgp9",
    "category": "vgp-assembly-v2",
    "workflow_repository": "assembly-decontamination-vgp9",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/richard-burhans/ncbi_fcs_adaptor/ncbi_fcs_adaptor/0.5.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ncbi_fcs_gx/ncbi_fcs_gx/0.5.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_dustmasker_wrapper/2.16.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/fasta_filter_by_length/fasta_filter_by_length/1.2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/ncbi_blast_plus/ncbi_blastn_wrapper/2.16.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/parse_mito_blast/parse_mito_blast/1.0.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0"
    ],
    "readme_cleaned": "Decontamination Workflow Decontamination (foreign contaminants and mitochondrial sequences) of genome assembly after scaffolding step. Part of the VGP Suite. Inputs - Genome assembly [fasta] - The NCBI taxonomic identifier of your species (e.g. 9606 for humans) - Your species binomial name (e.g. Homo Sapiens) - The assembly name (e.g. Hg19) - The haplotype being decontaminated - Maximum length of sequence to consider for mitochondrial scaffolds:. Select 0 to use all the scaffolds. Change this setting if you have particularly long scaffolds that cause BLAST to fail. Ouput - A taxonomy report containing the list of sequences and their identified taxonomy. [tabular] - Contaminant sequences [fasta] - List of mitochondrial scaffolds [txt] - An adaptor report with the adaptor sequences found in the assembly and the actions to perform to remove them [tabular] - A masking adaptor report, with the same informations than the adaptor report except the adaptor sequences found in the middle of the sequences are flagged for masking instead of trimming. [tabular] - Decontaminated assembly [fasta]",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Assembly-decontamination-VGP9/Assembly-decontamination-VGP9.ga"
  },
  {
    "workflow_id": "iwc_mitogenome-assembly-vgp0",
    "category": "vgp-assembly-v2",
    "workflow_repository": "mitogenome-assembly-vgp0",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/mitohifi/mitohifi/3.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compress_file/compress_file/0.1.0"
    ],
    "readme_cleaned": "Assembly of Mitochondrial DNA from PacBio HiFi reads Generate mitochondrial assembly based on PacBio HiFi reads. Part of the VGP suite, it can be run at any time independently of the other workflows. This workflow uses MitoHiFi and a mitochondrial reference to assemble the mitochondrial genome from PacBio reads. You do not need to provide the reference yourself, only the Latin name of the species. Inputs 1. Name of the Species 2. Name of the Assembly 3. Hifi long reads [fastq] 4. Email adress required for NCBI database query 5. Organism genetic code following NCBI table (for mitogenome annotation) Outputs 1. Contigs Statistics 2. Images : 1. Mitogenome Coverage 2. Mitogenome Annotation 3. Genbank file of the assembled mitogenome 4. Fasta file of the assembled mitogenome",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Mitogenome-assembly-VGP0/Mitogenome-Assembly-VGP0.ga"
  },
  {
    "workflow_id": "iwc_plot-nx-size",
    "category": "vgp-assembly-v2",
    "workflow_repository": "plot-nx-size",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "sort1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.9+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/add_value/addValue/1.0.1",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/mvdbeek/add_input_name_as_column/addName/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.5.1+galaxy2"
    ],
    "readme_cleaned": "Generate Nx and Size plot for multiple assemblies Generate Nx and size plots for multiple assemblies to compare the evolution of assembly quality through the scaffolding process. Inputs are the fasta files for each assembly to compare. Inputs Collection of fasta files. The name of each item in the collection will be used as labels for the Nx and Size plots. Outputs 1. Nx plot 2. Size plot",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Plot-Nx-Size/Generate-Nx-and-Size-plots-for-multiple-assemblies.ga"
  },
  {
    "workflow_id": "iwc_purge-duplicate-contigs-vgp6",
    "category": "vgp-assembly-v2",
    "workflow_repository": "purge-duplicate-contigs-vgp6",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/1.2.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "param_value_from_file",
      "Cut1",
      "cat1",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
      "__EXTRACT_DATASET__",
      "join1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
    ],
    "readme_cleaned": "Purge Duplicate Contigs Purge contigs marked as duplicates by purgedups (could be haplotypic duplication or overlap duplication). The contigs are purged from the first assembly (hap1, pri...), added to the second assembly (hp2, alt... ), then the 2nd assembly is purged as well. If you think only one of the assemblies needs purging, use the VGP6b workflow. This workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5) Inputs 1. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow) 2. Primary Assembly (hap1) [fasta] (Generated by the contigging workflow) 3. Alternate Assembly (hap2) [fasta] (Generated by the contigging workflow) 4. K-mer database [meryldb] (Generated by the k-mer profiling workflow) 5. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow) 6. Estimated Genome Size [txt] 7. Database for busco lineage (recommended: latest) 8. Lineage of your species for Busco Orthologs (recommended: vertebrata) 9. Name of first haplotype 10. Name of second haplotype Outputs 1. Haplotype 1 purged assembly (Fasta and gfa) 2. Haplotype 2 purged assembly (Fasta and gfa) 3. QC: BUSCO report for both assemblies 4. QC: Compleasm report for both assemblies 5. QC: Merqury report for both assemblies 6. QC: Assembly statistics for both assemblies 7. QC: Nx plot for both assemblies 8. QC: Size plot for both assemblies",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Purge-duplicate-contigs-VGP6/Purge-duplicate-contigs-VGP6.ga"
  },
  {
    "workflow_id": "iwc_purge-duplicates-one-haplotype-vgp6b",
    "category": "vgp-assembly-v2",
    "workflow_repository": "purge-duplicates-one-haplotype-vgp6b",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/purge_dups/purge_dups/1.2.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "param_value_from_file",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/merqury/merqury/1.3+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "__EXTRACT_DATASET__",
      "join1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2"
    ],
    "readme_cleaned": "Purge Duplicate Contigs Purge contigs marked as duplicates by purgedups in a single haplotype (could be haplotypic duplication or overlap duplication). If you think the purged contigs might belong to the other haplotype, use the workflow VGP6 instead. This workflow is the 6th workflow of the VGP pipeline. It is meant to be run after one of the contigging steps (Workflow 3, 4, or 5). Inputs 1. Genomescope model parameters [txt] (Generated by the k-mer profiling workflow) 2. Hifi long reads - trimmed [fastq] (Generated by Cutadapt in the contigging workflow) 3. Assembly to purge (e.g. hap1) [fasta] (Generated by the contigging workflow) 4. K-mer database [meryldb] (Generated by the k-mer profiling workflow) 5. Assembly to leave alone (used for merqury statistics) (e.g. hap2) [fasta] (Generated by the contigging workflow) 6. Estimated Genome Size [txt] 7. Database for busco lineage (recommended: latest) 8. Busco lineage (recommended: vertebrata) 9. Name of un-altered assembly 10. Name of purged assembly Outputs 1. Purged assembly (Fasta and gfa) 2. QC: BUSCO report for the purged assembly 3. QC: Compleasm report for the purged assembly 4. QC: Merqury report for both assemblies 5. QC: Assembly statistics for both assemblies 6. QC: Nx plot for both assemblies 7. QC: Size plot for both assemblies",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Purge-duplicates-one-haplotype-VGP6b/Purging-duplicates-one-haplotype-VGP6b.ga"
  },
  {
    "workflow_id": "iwc_scaffolding-bionano-vgp7",
    "category": "vgp-assembly-v2",
    "workflow_repository": "scaffolding-bionano-vgp7",
    "tool_names": [
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/bionano_scaffold/bionano_scaffold/3.7.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.3+galaxy1",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.4.0+galaxy1"
    ],
    "readme_cleaned": "Scaffolding with Bionano Scaffolding using Bionano optical map data Inputs 1. Bionano data [cmap] 2. Estimated genome size [txt] 3. Phased assembly generated by Hifiasm [gfa1] Outputs 1. Scaffolds 2. Non-scaffolded contigs 3. QC: Assembly statistics 4. QC: Nx plot 5. QC: Size plot",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Scaffolding-Bionano-VGP7/Scaffolding-BioNano-VGP7.ga"
  },
  {
    "workflow_id": "iwc_scaffolding-hic-vgp8",
    "category": "vgp-assembly-v2",
    "workflow_repository": "scaffolding-hic-vgp8",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.21+galaxy0",
      "__EXTRACT_DATASET__",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.7",
      "toolshed.g2.bx.psu.edu/repos/iuc/pretext_map/pretext_map/0.1.9+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bamtools_filter/bamFilter/2.5.2+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/pretext_snapshot/pretext_snapshot/0.0.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/yahs/yahs/1.2a.2+galaxy3",
      "Show tail1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_tail_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2_idx/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compleasm/compleasm/0.2.6+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/devteam/fasta_compute_length/fasta_compute_length/1.0.4",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bwa_mem2/bwa_mem2/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_point/ggplot2_point/3.5.1+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/imagemagick_image_montage/imagemagick_image_montage/7.1.2-2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_sort/samtools_sort/2.0.7",
      "toolshed.g2.bx.psu.edu/repos/iuc/pairtools_parse/pairtools_parse/1.1.3+galaxy6",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/pairtools_dedup/pairtools_dedup/1.1.3+galaxy6",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cut_tool/9.5+galaxy2"
    ],
    "readme_cleaned": "Scaffolding with HiC data This workflow performs genome assembly scaffolding using HiC data with YAHS. It is designed to be run as part of the VGP analysis trajectories, but can be used on any assembly in GFA format with Hi-C data. To generate a GFA from a fasta assembly, you can use the gfastats tool in Galaxy. Example of VGP trajectory : - VGP1: Kmer profiling - VGP4: Genome assembly with HiC phasing - VGP6: Purge duplicated haplotigs - VGP8: Scaffolding with HiC Inputs 1. Genome assembly [gfa] 2. Haplotype being scaffolded (Will be added to scaffold names: e.g. >scaffold01H1) 3. HiC reads paired collection [fastq] 5. Trim Hi-C data? If yes, trim five bases at the beginning of each read. Use with Arima Hi-C data if the Hi-C map looks \"noisy\" and the reads haven't been trimmed before. 6. Minimum Mapping Quality [int] (Default:20). Minimum mapping quality for Hi-C alignments. Set to 0 if you want no filtering. 6. Database for busco lineage (recommended: latest) 7. Busco lineage (recommended for VGP data: vertebrata) 8. Restriction enzyme sequence (recommended for VGP data: Arima Hi-C 2.0) 9. Estimated genome size [txt] (Output from the contigging workflows 3,4, or 5). A simple text file containing the estimated genome size as an integer. E.g. 2288021 Outputs 1. Scaffolds in [fasta] and [gfa] format with the haplotype in the scaffold names. 2. If you selected yes for Hi-C trimming, the trimmed collections of Hi-C reads. 3. QC: Assembly statistics. 4. QC: Hi-C duplications statistics. 5. QC: Nx plot. 6. QC: Size plot. 7. QC: Compleasm report. 8. QC: Pretext Maps before and after scaffolding. 9. QC: Statistics on Hi-C alignements before and after scaffolding",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/Scaffolding-HiC-VGP8/Scaffolding-HiC-VGP8.ga"
  },
  {
    "workflow_id": "iwc_hi-c-contact-map-for-assembly-manual-curation",
    "category": "vgp-assembly-v2",
    "workflow_repository": "hi-c-contact-map-for-assembly-manual-curation",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/gfastats/gfastats/1.3.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_telo/1.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.28+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/pretext_map/pretext_map/0.1.9+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bam_coverage/deeptools_bam_coverage/3.5.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.31.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/pretext_graph/pretext_graph/0.0.7+galaxy0",
      "Filter1",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/pretext_snapshot/pretext_snapshot/0.0.4+galaxy0",
      "__EXTRACT_DATASET__"
    ],
    "readme_cleaned": "Hi-C Contact map generation for manual curation of genome assemblies This workflow generates Hi-C contact maps for diploid genome assemblies in the Pretext format. It includes tracks for PacBio read coverage, Gaps, and telomeres. The Pretext files can be open in PretextView for the manual curation of genome assemblies. Inputs 1. Haplotype 1 [fasta] 2. Will you use a second haplotype? 3. Haplotype 2 [fasta] 4. Do you want to add suffixes to the scaffold names? Select yes if the scaffold names in your assembly do not contain haplotype information. 5. Haplotype 1 suffix This suffix will be added to haplotype 1 scaffold names if you selected to add suffixes to the scaffold names. 6. Haplotype 2 suffix This suffix will be added to haplotype 2 scaffold names if you selected to add suffixes to the scaffold names. 7. Hi-C reads [fastq] Paired Collection containing the Hi-D data 8. Do you want to trim the Hi-C data? If yes, remove 5bp at the end of Hi-C reads. Use with Arima Hi-C data if the Hi-C map looks \"noisy\". 9. Minimum Mapping Score Minimum mapping score to keep for Hi-C alignments in the filtered PretextMap. Set to 0 to keep all mapped reads. Default: 20 . 10. Telomere repeat to suit species Expected value of the repeated sequences in the telomeres. Default value [CCCTAA] is suited to vertebrates. 11. PacBio reads [fastq] Collection of PacBio reads. Outputs 1. Concatenated Assembly [fasta] If two haplotypes are used. 2. Trimmed Hi-C data (If trimming option is selected) [fastq] 3. Mapped Hi-C reads [bam] 4. Telomeres track [bedgraph] 5. Gap track [bedgraph] 6. Coverage track [bigwig] 7. Gaps in coverage track [bedgraph] 7. Pretext Map without tracks [pretext], filtered and unfiltered. 8. Pretext Map with tracks [pretext], filtered and unfiltered. 9. Pretext Snapshot image of the Hi-C contact map [png], filtered and unfiltered.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/hi-c-contact-map-for-assembly-manual-curation/hi-c-map-for-assembly-manual-curation.ga"
  },
  {
    "workflow_id": "iwc_kmer-profiling-hifi-vgp1",
    "category": "vgp-assembly-v2",
    "workflow_repository": "kmer-profiling-hifi-vgp1",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_count_kmers/meryl_count_kmers/1.3+galaxy7",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_groups_kmers/meryl_groups_kmers/1.3+galaxy7",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_histogram_kmers/meryl_histogram_kmers/1.3+galaxy7",
      "toolshed.g2.bx.psu.edu/repos/iuc/genomescope/genomescope/2.0.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/imagemagick_image_montage/imagemagick_image_montage/7.1.2-2+galaxy0",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval/0.0.7+galaxy5",
      "toolshed.g2.bx.psu.edu/repos/richard-burhans/rdeval/rdeval_report/0.0.7+galaxy5"
    ],
    "readme_cleaned": "VGP Workflow 1 This workflow produces a Meryl database and Genomescope outputs that will be used to determine parameters for following workflows, and assess the quality of genome assemblies. Specifically, it provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It also provides statistics on the PacBio Hifi reads. Inputs 1. The name of the species being assembled 2. The Name of the assembly 3. A collection of Hifi long reads in FASTQ format 4. k-mer length 5. Ploidy Outputs - Meryl Database of k-mer counts - GenomeScope - Linear plot - Log plot - Transformed linear plot - Transformed log plot - Summary - Model - Model parameteres - RDeval for PacBio Hifi Reads QC - Reads statistics - HTML report - Mash QC, heatmap of distance between Hifi datasets",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/kmer-profiling-hifi-VGP1/kmer-profiling-hifi-VGP1.ga"
  },
  {
    "workflow_id": "iwc_kmer-profiling-hifi-trio-vgp2",
    "category": "vgp-assembly-v2",
    "workflow_repository": "kmer-profiling-hifi-trio-vgp2",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_count_kmers/meryl_count_kmers/1.3+galaxy7",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl/meryl/1.3+galaxy6",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_groups_kmers/meryl_groups_kmers/1.3+galaxy7",
      "toolshed.g2.bx.psu.edu/repos/iuc/genomescope/genomescope/2.0.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/meryl_histogram_kmers/meryl_histogram_kmers/1.3+galaxy7"
    ],
    "readme_cleaned": "VGP Workflow 1 This workflow collects the metrics on the properties of the genome under consideration by analyzing the k-mer frequencies. It provides information about the genomic complexity, such as the genome size and levels of heterozygosity and repeat content, as well about the data quality. It uses reads from two parental genomes to partition long reads from the offspring into haplotype-specific k-mer databases. Inputs - Collection of Hifi long reads [fastq] (Collection) - Paternal short-read Illumina sequencing reads [fastq] (Collection) - Maternal short-read Illumina sequencing reads [fastq] (Collection) - k-mer length - Ploidy Outputs - Meryl databases of k-mer counts - Child - Paternal haplotype - Maternal haplotype - GenomeScope metrics for child and the two parental genomes (three GenomeScope profiles in total) - Linear plot - Log plot - Transformed linear plot - Transformed log plot - Summary - Model - Model parameteres ![image](https://github.com/galaxyproject/iwc/assets/4291636/35282f8e-d021-44f6-8e03-7b58b32d6d00)",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/VGP-assembly-v2/kmer-profiling-hifi-trio-VGP2/kmer-profiling-hifi-trio-VGP2.ga"
  },
  {
    "workflow_id": "iwc_dada2",
    "category": "amplicon",
    "workflow_repository": "dada2",
    "tool_names": [
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_plotqualityprofile/dada2_plotQualityProfile/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_filterandtrim/dada2_filterAndTrim/1.34.0+galaxy0",
      "__UNZIP_COLLECTION__",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_learnerrors/dada2_learnErrors/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_dada/dada2_dada/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_mergepairs/dada2_mergePairs/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_makesequencetable/dada2_makeSequenceTable/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_removebimeradenovo/dada2_removeBimeraDenovo/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_seqcounts/dada2_seqCounts/1.34.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/dada2_assigntaxonomyaddspecies/dada2_assignTaxonomyAddspecies/1.34.0+galaxy0"
    ],
    "readme_cleaned": "Dada2: amplicon analysis for paired end data Inputs dataset - Paired input data paired input collection in FASTQ format Inputs values - Read length forward/reverse reads length of the forward/reverse reads to which they should be truncated in the filter and trim step - Pool samples pooling may increase sensitivity - Reference database that should be used for taxonomic assignment Processing The workflow follows the steps described in the [dada2 tutorial](https://benjjneb.github.io/dada2/tutorial.html). As a first step the input collection is sorted. This is important because the dada2 step outputs a collection in sorted order. If the input collection would not be sorted then the mergePairs step samples would be mixed up. - FilterAndTrim Quality control by filtering and trimming reads - QualityProfile is called before and after the FilterAndTrim step - Unzip Collection separates forward and reverse reads (the next steps are evaluated separately on forward and reverse reads) - learnErrors learn error rates - dada filter noisy reads - mergePairs merge forward and reverse reads - makeSequenceTable create the sequence table - removeBimeraDenovo remove chimeric sequencs - assignTaxonomy assign taxonomic information from a reference data base TODO Some possibilities to extend/improve the workflow - output BIOM - use ASV1, ... in sequence table and taxonomy output, and output additional fasta - allow to use custom taxonomy / make it optional",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/amplicon/dada2/dada2_paired.ga"
  },
  {
    "workflow_id": "iwc_amr_gene_detection",
    "category": "bacterial_genomics",
    "workflow_repository": "amr_gene_detection",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/staramr/staramr_search/0.11.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/amrfinderplus/amrfinderplus/3.12.8+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/abricate/abricate/1.0.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.0+galaxy1"
    ],
    "readme_cleaned": "AMR gene detection workflow in an assembled bacterial genome This workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps: 1. Genomic detection - Antimicrobial resistance gene identification: - StarAMR to blast against ResFinder, PointFinder and PlasmidFinder database - AMRFinderPlus to find antimicrobial resistance genes and point mutations - Virulence gene identification: - ABRicate with VFDBA database 2. Aggregating outputs into a single JSON file - ToolDistillator to extract and aggregate information from different tool outputs to JSON parsable files Inputs 1. Assembled bacterial genome in fasta format. Outputs 1. Genomic detection - Antimicrobial resistance gene identification: - AMR gene list - MLST typing - Plasmid gene identification - Blast hits - AMR gene fasta (assembled nucleotide sequences) - Point mutation list - Virulence gene identification: - Gene identification in tabular format 2. Aggregating outputs: - JSON file with information about the outputs of StarAMR, AMRFinderPlus, ABRicate",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/bacterial_genomics/amr_gene_detection/amr_gene_detection.ga"
  },
  {
    "workflow_id": "iwc_bacterial-quality-and-contamination-control-post-assembly",
    "category": "bacterial_genomics",
    "workflow_repository": "bacterial-quality-and-contamination-control-post-assembly",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/checkm2/checkm2/1.0.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/2.1.3+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bracken/est_abundance/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/recentrifuge/recentrifuge/1.16.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.0+galaxy1"
    ],
    "readme_cleaned": "Bacterial quality and Contamination control post assembly workflow This workflow uses paired-end illumina fastq(.gz) files and executes the following steps: 1. Genome assembly quality control - Quast to assess genome quality - Checkm2 to predict the completeness and contamination in an assembly 2. Taxonomic assignment on contigs - Kraken2 assignment - Bracken to re-estimate abundance to the species level - Recentrifuge to make a krona chart 3. Aggregating outputs into a single JSON file - ToolDistillator to extract and aggregate information from different tool outputs to JSON parsable files Inputs 1. Fasta as contigs from raw reads assembly. Optional : Paired-end illumina raw reads in fastq(.gz) format. (Used with Quast) Outputs 1. Genome assembly quality: - quality reports 2. Taxonomic assignation: - Tabular report of identified species - Tabular file with assigned read to a taxonomic level - Krona chart to illustrate species diversity of the sample 3. Aggregating outputs: - JSON file with information about the outputs of Quast, Checkm2, Kraken2, Bracken, Recentrifuge",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/bacterial_genomics/bacterial-quality-and-contamination-control-post-assembly/bacterial_quality_and_contamination_control_post_assembly.ga"
  },
  {
    "workflow_id": "iwc_bacterial_genome_annotation",
    "category": "bacterial_genomics",
    "workflow_repository": "bacterial_genome_annotation",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/isescan/isescan/1.7.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/integron_finder/integron_finder/2.0.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/plasmidfinder/plasmidfinder/2.1.6+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/1.9.4+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.0+galaxy1"
    ],
    "readme_cleaned": "Bacterial genome annotation workflow (v1.1.8) This workflow uses assembled bacterial genome fasta files (but can be any fasta file) and executes the following steps: 1. Genomic annotation - Bakta to predict CDS and small proteins (sORF) 2. Integron identification - IntegronFinder2 to identify CALIN elements, In0 elements, and complete integrons 3. Plasmid gene identification - Plasmidfinder to identify and typing plasmid sequences 4. Inserted sequence (IS) detection - ISEScan to detect IS elements 5. Aggregating outputs into a single JSON file - ToolDistillator to extract and aggregate information from different tool outputs to JSON parsable files Inputs 1. Assembled bacterial genome in fasta format. Outputs 1. Genomic annotation: - genome annotation in tabular, gff and several other formats - annotation plot - nucleotide and protein sequences identified - summary of genomic identified elements 2. Integron identification: - integron identification in tabular format and a summary 3. Plasmid gene identification: - plasmid gene identified and associated blast hits 4. Inserted Element (IS) detection: - IS element list in tabular format - is hits in fasta format - ORF hits in protein and nucleotide fasta format - IS annotation gff format 5. Aggregating outputs: - JSON file with information about the outputs of Bakta, IntegronFinder2, Plasmidfinder, ISEScan",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/bacterial_genomics/bacterial_genome_annotation/bacterial_genome_annotation.ga"
  },
  {
    "workflow_id": "iwc_fragment-based-docking-scoring",
    "category": "computational-chemistry",
    "workflow_repository": "fragment-based-docking-scoring",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/ctb_frankenstein_ligand/ctb_frankenstein_ligand/2013.1-0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/enumerate_charges/enumerate_charges/2020.03.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbcavity/rxdock_rbcavity/2013.1.1_148c5bd1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/rxdock_rbdock/rxdock_rbdock/2013.1.1_148c5bd1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/4.2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/sucos_docking_scoring/sucos_docking_scoring/2020.03.4+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/rdock_sort_filter/rdock_sort_filter/2013.1-0+galaxy0"
    ],
    "readme_cleaned": "Fragment-based virtual screening with docking and pose scoring Dock a compound library against a target protein with rDock and validate the poses generated against a reference fragment using SuCOS to compare the feature overlap. Poses are filtered by a user-specified SuCOS threshold. A list of fragments should be specified which will be used to define the cavity for docking, using the 'Frankenstein ligand' technique. For more details, please see https://www.informaticsmatters.com/blog/2018/11/23/cavities-and-frankenstein-molecules.html Compounds are split into collections and then recombined to allow the workflow to be run in a highly parallelized fashion. To specify the level of parallelization, use the 'Collection size' parameter.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/computational-chemistry/fragment-based-docking-scoring/fragment-based-docking-scoring.ga"
  },
  {
    "workflow_id": "iwc_gromacs-dctmd",
    "category": "computational-chemistry",
    "workflow_repository": "gromacs-dctmd",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/get_online_data/ctb_online_data_fetch/0.4",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/1.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/add_line_to_file/add_line_to_file/0.1.0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/2022+galaxy0",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_makendx/gmx_makendx/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/1.1.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/chemteam/biomd_neqgamma/biomd_neqgamma/0.1.5.2+galaxy1"
    ],
    "readme_cleaned": "GROMACS dcTMD free energy calculation Perform an ensemble of targeted MD simulations of a user-specified size using the GROMACS PULL code and calculate dcTMD free energy and friction profiles for the resulting dissocation pathway. Note that pathway separation is not performed by the workflow; the user is responsible for checking the ensemble themselves. The input protein (PDB) and ligand (SDF) files provided are parameterized by the 'Protein-ligand complex parameterization' subworkflow. Note that the workflow uses a MDP file for configuring the TMD simulations; this is packaged alongside the workflow as tmd.mdp. Citations Steffen Wolf and Gerhard Stock (2018), Targeted Molecular Dynamics Calculations of Free Energy Profiles Using a Nonequilibrium Friction Correction, J. Chem. Theory Comput. doi:10.1021/acs.jctc.8b00835 Steffen Wolf, Benjamin Lickert, Simon Bray and Gerhard Stock (2020), Multisecond ligand dissociation dynamics from atomistic simulations, Nat. Commun. doi:10.1038/s41467-020-16655-1",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/computational-chemistry/gromacs-dctmd/gromacs-dctmd.ga"
  },
  {
    "workflow_id": "iwc_gromacs-mmgbsa",
    "category": "computational-chemistry",
    "workflow_repository": "gromacs-mmgbsa",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/1.1.0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_editconf/gmx_editconf/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.1",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_solvate/gmx_solvate/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_em/gmx_em/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/parmconv/parmconv/21.10+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_sim/gmx_sim/2022+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/md_converter/md_converter/1.9.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/mmpbsa_mmgbsa/mmpbsa_mmgbsa/21.10+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/1.1.1",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "Cut1",
      "Summary_Statistics1"
    ],
    "readme_cleaned": "GROMACS MMGBSA free energy calculation Perform an ensemble of MD simulations of a user-specified size using GROMACS, and calculate MMGBSA free energies using AmberTools. An ensemble average is calculated and returned to the user as the final input. The input protein (PDB) and ligand (SDF) files provided are parameterized by the 'Protein-ligand complex parameterization' subworkflow.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/computational-chemistry/gromacs-mmgbsa/gromacs-mmgbsa.ga"
  },
  {
    "workflow_id": "iwc_protein-ligand-complex-parameterization",
    "category": "computational-chemistry",
    "workflow_repository": "protein-ligand-complex-parameterization",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/ctb_rdkit_descriptors/ctb_rdkit_descriptors/2020.03.4+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_setup/gmx_setup/2021.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/1.1.1",
      "Cut1",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_antechamber/ambertools_antechamber/21.10+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/ambertools_acpype/ambertools_acpype/21.10+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/chemteam/gmx_merge_topology_files/gmx_merge_topology_files/3.4.3+galaxy0"
    ],
    "readme_cleaned": "Protein-ligand complex parameterization Parameterizes an input protein (PDB) and ligand (SDF) file prior to molecular dynamics simulation with GROMACS. This is a simple workflow intended for use as a subworkflow in more complex MD workflows. It is used as a subworkflow by the GROMACS MMGBSA and dcTMD workflows.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/computational-chemistry/protein-ligand-complex-parameterization/protein-ligand-complex-parameterization.ga"
  },
  {
    "workflow_id": "iwc_parallel-accession-download",
    "category": "data-fetching",
    "workflow_repository": "parallel-accession-download",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy0",
      "__APPLY_RULES__"
    ],
    "readme_cleaned": "Parallel Accession Download Downloads fastq files for sequencing run accessions provided in a text file using fasterq-dump. Creates one job per listed run accession, and is therefore much faster and more robust to errors when many accessions need to be downloaded.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/data-fetching/parallel-accession-download/parallel-accession-download.ga"
  },
  {
    "workflow_id": "iwc_sra-manifest-to-concatenated-fastqs",
    "category": "data-fetching",
    "workflow_repository": "sra-manifest-to-concatenated-fastqs",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/sra_tools/fasterq_dump/3.1.1+galaxy0",
      "__RELABEL_FROM_FILE__",
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/artbio/concatenate_multiple_datasets/cat_multi_datasets/1.4.3"
    ],
    "readme_cleaned": "SRA manifest to concatenated fastqs This workflow takes as input a SRA manifest from SRA Run Selector (or a tabular with a header line), downloads all sequencing run data from the SRA and arranges it into per-sample fastq or pairs of fastq datasets. It will work out the relationship between runs and samples from the user-indicated run and sample columns in the input and will concatenate sequencing run data as needed to obtain per-sample datasets. Input dataset - The workflow needs a single tabular input dataset, which is supposed to list SRA run identifiers in one column and sample names in another, and which needs to have a header line. - SRA manifests obtained via the SRA Run Selector and turned into tabular format represent valid input. Input values - Column number with SRA run ID For manifests obtained through the SRA Run Selector this is column 1 - Column number with sample names The number of the column that should be used to assign sequencing runs to samples The names in the column will also serve as the labels of datasets in the output collection. For manifests obtained through the SRA Run Selector suitable columns might be number 6 (BioSample), 16 (Experiment) or 36 (Sample Name). Processing - The workflow downloads sequencing run data in fastq format with fasterqdump (one job per SRA run ID). - Run data gets concatenated if it comes from the same sample. Outputs - There are 2 outputs, one with paired-end datasets, one with single-read datasets. Limitations - Special characters in sample names (anything that is not an English alphabet character, digit, underscore, dash, space, dot or comma ([a-zA-Z0-9\\- \\.,]) will be converted to dashes (-).",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/data-fetching/sra-manifest-to-concatenated-fastqs/sra-manifest-to-concatenated-fastqs.ga"
  },
  {
    "workflow_id": "iwc_atacseq",
    "category": "epigenetics",
    "workflow_repository": "atacseq",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.9+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bamtools_filter/bamFilter/2.5.2+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_idxstats/samtools_idxstats/2.0.5",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/pe_histogram/pe_histogram/1.0.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "wig_to_bigWig",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_slopbed/2.31.1+galaxy0",
      "param_value_from_file",
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_mergebed/2.31.1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_coveragebed/2.31.1+galaxy0",
      "cat1",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.24.1+galaxy0"
    ],
    "readme_cleaned": "ATAC-seq Analysis: Chromatin Accessibility Profiling This workflow is highly concordant with the corresponding training material. You can have more information about ATAC-seq analysis in the [slides](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/slides.html) and the [tutorial](https://training.galaxyproject.org/training-material/topics/epigenetics/tutorials/atac-seq/tutorial.html). Inputs dataset - The workflow needs a single input which is a list of dataset pairs of fastqsanger. Inputs values - referencegenome: this field will be adapted to the genomes available for bowtie2 and the genomes available for bedtools slopbed (dbkeys table) - effectivegenomesize: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes) - binsize: this is used when normalization of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs. Processing - The workflow will remove nextera adapters and low quality bases and filter out any read smaller than 15bp. - The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb. - The BAM is filtered to keep only MAPQ30, concordant pairs and pairs outside of the mitochondria. - The PCR duplicates are removed with Picard (only from version 0.8). - The BAM is converted to BED to enable macs2 to take both pairs into account. - The peaks are called with macs2 which at the same time generates a coverage file. - The coverage file is converted to bigwig - The amount of reads 500bp from summits and the total number of reads are computed. - Two normalizations are computed: - By million reads - By million reads in peaks (500bp from summits) - Other QC are performed: - A histogram with fragment length is computed. - The evaluation of percentage of reads to chrM or MT is computed. - A multiQC is run to have an overview of the QC. Warning - The referencegenome parameter value is used to select references in bowtie2 and bedtools slopbed. Only references that are present in bowtie2 and bedtools slopbed are selectable. If your favorite reference genome is not available ask your administrator to make sure that each bowtie2 reference has a corresponding len file for use in bedtools slopbed.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/atacseq/atacseq.ga"
  },
  {
    "workflow_id": "iwc_average-bigwig-between-replicates",
    "category": "epigenetics",
    "workflow_repository": "average-bigwig-between-replicates",
    "tool_names": [
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0"
    ],
    "readme_cleaned": "BigWig Replicates Averaging Workflow This workflow is very useful when you processed multiple samples in collections and you want to generate an average coverage per condition. Inputs dataset - The workflow needs a single input which is a list of bigwigs (normalized). The identifiers of your bigwigs must be like: - whateversample1identificationOfReplicate1 - whateversample1identificationOfReplicate2 - ... - whateversample2identificationOfReplicate1 - whateversample2identificationOfReplicate2 - ... Inputs values - binsize: this is used when average of coverage is performed. Large values will allow to have smaller output files but with less resolution while small values will increase computation time and size of output files to produce more resolutive bigwigs. I suggest 5bp for RNA-seq and 50bp for other applications. Processing - The workflow will split identifiers between everything which is before the last underscore which will be the sample and everything which is after the last underscore which will be the replicate identifier. And restructure the collection as list:list: - whateversample1: - identificationOfReplicate1 - identificationOfReplicate2 - ... - whateversample2: - identificationOfReplicate1 - identificationOfReplicate2 - --- - ... - Then it will average bigwigs into each inner list Outputs - The output is a collection of bigwig datasets like: - whateversample1 - whateversample2 - ...",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/average-bigwig-between-replicates/average-bigwig-between-replicates.ga"
  },
  {
    "workflow_id": "iwc_chipseq-pe",
    "category": "epigenetics",
    "workflow_repository": "chipseq-pe",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "wig_to_bigWig",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
    ],
    "readme_cleaned": "ChIP-seq Analysis: Paired-End Read Processing Inputs dataset - The workflow needs a single input which is a list of dataset pairs of fastqsanger. Inputs values - adapters sequences: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera. - referencegenome: this field will be adapted to the genomes available for bowtie2. - effectivegenomesize: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes). - normalizeprofile: Whether you want to have a profile normalized as Signal to Million Fragments. Processing - The workflow will remove illumina adapters and low quality bases and filter out any pair with mate smaller than 15bp. - The filtered reads are mapped with bowtie2 with default parameters. - The BAM is filtered to keep only MAPQ30 and concordant pairs. - The peaks are called with MACS2 which at the same time generates a coverage file (normalized or not). - The coverage is converted to bigwig. - A MultiQC is run to have an overview of the QC. Warning - The filtered bam still has PCR duplicates which are removed by MACS2. Contribution @lldelisle wrote the workflow. @nagoue updated the tools, made it work in usegalaxy.org, fixed the best practices and wrote the tests.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/chipseq-pe/chipseq-pe.ga"
  },
  {
    "workflow_id": "iwc_chipseq-sr",
    "category": "epigenetics",
    "workflow_repository": "chipseq-sr",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/5.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.5+galaxy2",
      "wig_to_bigWig",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
    ],
    "readme_cleaned": "ChIP-seq Analysis: Single-End Read Processing Inputs dataset - The workflow needs a single input which is a list of fastqsanger files. Inputs values - adapters sequenceforward: this depends on the library preparation. If you don't know, use FastQC to determine if it is Truseq or Nextera. - referencegenome: this field will be adapted to the genomes available for bowtie2. - effectivegenomesize: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes). - normalizeprofile: Whether you want to have a profile normalized as Signal to Million Reads. Processing - The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp. - The filtered reads are mapped with bowtie2 with default parameters. - The BAM is filtered to keep only MAPQ30. - The peaks are called with MACS2 with a fixed extension of 200bp which at the same time generates a coverage file (normalized or not). - The coverage is converted to bigwig. - A MultiQC is run to have an overview of the QC. Warning - The filtered bam still has PCR duplicates which are removed by MACS2.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/chipseq-sr/chipseq-sr.ga"
  },
  {
    "workflow_id": "iwc_consensus-peaks",
    "category": "epigenetics",
    "workflow_repository": "consensus-peaks",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_multiintersectbed/2.31.1",
      "wig_to_bigWig",
      "toolshed.g2.bx.psu.edu/repos/iuc/table_compute/table_compute/1.2.4+galaxy0",
      "wc_gnu",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/deeptools_bigwig_average/deeptools_bigwig_average/3.5.4+galaxy0",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.31.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sorted_uniq/9.3+galaxy1"
    ],
    "readme_cleaned": "Consensus Peak Calling for ChIP-seq, ATAC-seq and CUT&RUN Replicates The goal of this workflow is to get a list of confident peaks with summits from n replicates. Inputs dataset - The workflow needs a single input which is a list of datasets with n BAM where PCR duplicates have been removed (the workflow also works for nested list if you have multiple conditions each with multiple replicates). Inputs values - Minimum number of overlap: Minimum number of replicates into which the final summit should be present. - effectivegenomesize: this is used by MACS2 and may be entered manually (indications are provided for heavily used genomes). - binsize: this is the bin sized used to compute the average of normalized profiles. Large values will allow to have a smaller output file but with less resolution while small values will increase computation time and size of the output file to produce a more resolutive bigwig. Strategy summary Here is a generated example to highlight the strategy: ![strategy](https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/strategy.png) Processing - The workflow will: - first part: - call peaks and compute normalized coverage on each BAM individually - average normalized profiles - compute the intersection between all peaks and filter when at least x replicate overlaps - second part: - subset all BAM to get the same number of reads - call peaks on all subsetted BAM combined - finally, keep only peaks from the second part that have summits overlapping the filtered intersection of the first part.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/consensus-peaks/consensus-peaks-atac-cutandrun.ga"
  },
  {
    "workflow_id": "iwc_cutandrun",
    "category": "epigenetics",
    "workflow_repository": "cutandrun",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lparsons/cutadapt/cutadapt/4.9+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtool_filter2/samtool_filter2/1.8+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_bamtobed/2.31.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/macs2/macs2_callpeak/2.2.9.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_grep_tool/9.3+galaxy1",
      "wig_to_bigWig",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0"
    ],
    "readme_cleaned": "CUT&RUN/CUT&TAG Analysis: Protein-DNA Interaction Mapping Inputs dataset - The workflow needs a single input which is a list of dataset pairs of fastqsanger. Inputs values - adapter sequences: this depends on the library preparation. Usually CUT&RUN is Truseq and CUT&TAG is Nextera. If you don't know, use FastQC to determine if it is Truseq or Nextera - referencegenome: this field will be adapted to the genomes available for bowtie2 - effectivegenomesize: this is used by macs2 and may be entered manually (indications are provided for heavily used genomes) - normalizeprofile: Whether you want to have a profile normalized as Signal to Million Reads. Processing - The workflow will remove illumina adapters and low quality bases and filter out any read smaller than 15bp - The filtered reads are mapped with bowtie2 allowing dovetail and fragment length up to 1kb - The BAM is filtered to keep only MAPQ30 and concordant pairs - The PCR duplicates are removed with Picard (only from version 0.6) - The BAM is converted to BED to enable macs2 to take both pairs into account - The peaks are called with macs2 which at the same time generates a coverage file (normalized or not). - The coverage file is converted to bigwig - A multiQC is run to have an overview of the QC",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/cutandrun/cutandrun.ga"
  },
  {
    "workflow_id": "iwc_hic-hicup-cooler",
    "category": "epigenetics",
    "workflow_repository": "hic-hicup-cooler",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/lldelisle/cooler_csort_tabix/cooler_csort_tabix/0.9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/pygenometracks/pygenomeTracks/3.8+galaxy2"
    ],
    "readme_cleaned": "(Capture) Hi-C Processing: FASTQ to Balanced Cool Files This can also be used for Hi-ChIP experiments, in that case the output with matrix with iced values is ignored and the matrix to use is matrix with raw values. Input datasets - The workflow needs a list of dataset pairs of fastqsanger. Input values - genome name: suggested from the bowtie2 indices, it is used to map and build the list of bins. - restriction enzyme: Restriction enzyme used e.g. A^GATCT,BglII. The '^' is used to express where the enzyme cuts. - No fill-in: If you used a biotin fill-in protocol, put this to false, else, put it to true. - minimum MAPQ: Filtering to apply to pairs you want to keep in your matrix, set it to 0 to not apply filtering (HiCUP already filter for uniquely mapped or MAPQ30). - Bin size in bp: Used to generate your first matrix but you will be able to rerun the subworkflow hictabixtocoolcooler to get other resolutions. - Interactions to consider to calculate weights in normalization step: this is a parameter for the last correction step (ICE). For the region capture workflow: - chromosome, start and end positions of the capture region For the Hi-C workflow: - region to use in pyGenomeTracks to check the matrices. Processing - Reads are processed with HiCUP which comprises these steps: - Truncation of reads for the religation motif - Mapping of mates independently with bowtie2 - Pairing the mates when both mates are uniquely mapped or MAPQ30 - Filtering the pairs for undigested, self-ligated... - Removing duplicates - The output BAM file is converted to medium juicer format: where str = strand (0 for forward, anything else for reverse) and pos is the middle of the fragment. - The pairs are filtered for MAPQ if specified. - For the region capture Hi-C workflow the pairs are filtered for both mates in the captured region. - The filtered pairs are sorted and indexed with coolercsort. - The pairs are loaded into a matrix of the given resolution and balanced with cooler. - A final plot is made with pyGenomeTracks using the balanced matrices on the region provided or the capture region. Subworkflows There are 2 subworkflows: hictabixtocoolcooler and hicfastqtopairshicup.ga. hictabixtocoolcooler This first subworkflow can be used to generate matrices to different resolutions using one of the output of the full workflow (valid pairs filtered and sorted). If the dataset are still in galaxy (format: juicermediumtabix.gz), the workflow can be run directly. If the dataset is not anymore in galaxy, you need to upload and specify the datatype as: juicermediumtabix.gz hicfastqtopairshicup The second subworkflow has no real reason to be launched by itself except for QC tests. If you want to run the first subworkflow from these results: - You first need to filter the pairs (valid pairs in juicebox format MAPQ filtered) for the capture region if relevent using the tool Filter1 (Filter data on any column using simple expressions) with the condition (c3=='chr2' and c4 170000000) and (c7==\"chr2\" and c8 170000000) if your capture region is chr2:170000000-180000000. - Then you need to run coolercsort (cooler csort with tabix Sort and index a contact list.) with as input the valid pairs in juicebox format MAPQ filtered or the output of the previous step and for \"Format of your input file\" use \"Juicer Medium Format\". The output of coolercsort can be used as input of the first subworkflow.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/epigenetics/hic-hicup-cooler/chic-fastq-to-cool-hicup-cooler.ga"
  },
  {
    "workflow_id": "iwc_assembly-with-flye",
    "category": "genome-assembly",
    "workflow_repository": "assembly-with-flye",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/flye/flye/2.9.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4"
    ],
    "readme_cleaned": "Genome assembly with Flye workflow Why use this workflow? - This is a fairly simple workflow that assembles a genome from long sequencing reads. - It takes in sequencing reads from PacBio (Hifi or non-Hifi), or Oxford Nanopore. - If you have PacBio Hifi reads, you may prefer to use a workflow with the assembly tool Hifiasm, such as the those in the suite of VGP workflows. Inputs Raw sequencing reads from PacBio or Oxford Nanopore in format: fasta, fasta.gz, fastq, fastq.gz, fastqsanger.gz or fastqsanger What does the workflow do - Assembles the reads with the tool Flye - Summarizes the statistics with the tool Fasta statistics - Report with the tool Quast - Renders the assembly graph with the tool Bandage Settings Run as-is or change parameters at runtime For example: - change the Flye option of \"mode\" to the correct sequencing type - change the Quast option for \"Type of organism\" to correct taxon Outputs - Flye assembly output - four files: fasta, gfa for bandage, graphdot file, assembly info - Fasta statistics - Bandage image - Quast report",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome-assembly/assembly-with-flye/Genome-assembly-with-Flye.ga"
  },
  {
    "workflow_id": "iwc_bacterial-genome-assembly",
    "category": "genome-assembly",
    "workflow_repository": "bacterial-genome-assembly",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/shovill/shovill/1.1.0+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_info/2022.09+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/bandage/bandage_image/2022.09+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.0+galaxy1"
    ],
    "readme_cleaned": "Bacterial genome assembly workflow for paired end data This workflow uses paired-end illumina trimmed reads fastq(.gz) files and executes the following steps: 1. Assembly raw reads to a final contig fasta file - Shovill 2. Assembly visualization and statistics - Bandage to plot assembly graph 3. Aggregating outputs into a single JSON file - ToolDistillator to extract and aggregate information from different tool outputs to JSON parsable files Inputs 1. Paired-end illumina trimmed reads in fastq(.gz) format. Outputs 1. Assembly: - Assembly with contig in fasta - Mapped read on assembly in bam format - Graph assembly in gfa format 2. Visualization of Assembly: - Assembly Graph - Assembly report 3. Aggregating outputs - JSON file with information about the outputs of Shovill, Bandage",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome-assembly/bacterial-genome-assembly/bacterial_genome_assembly.ga"
  },
  {
    "workflow_id": "iwc_polish-with-long-reads",
    "category": "genome-assembly",
    "workflow_repository": "polish-with-long-reads",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.26+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/racon/racon/1.5.0+galaxy1"
    ],
    "readme_cleaned": "Assembly polishing with Racon workflow Inputs - Sequencing reads in format: fastq, fastq.gz, fastqsanger.gz or fastqsanger - Genome assembly to be polished, in fasta format What does the workflow do - After long reads have been assembled into a genome (contigs), this can be polished with the same long reads. - This workflow uses the tool minimap2 to map the long reads back to the assembly, and then uses Racon to make polishes. - This is repeated a further 3 times. In more detail: - minimap2 : long reads are mapped to assembly => overlaps.paf. - overaps, long reads, assembly => Racon => polished assembly 1 - using polished assembly 1 as input; repeat minimap2 + racon => polished assembly 2 - using polished assembly 2 as input, repeat minimap2 + racon => polished assembly 3 - using polished assembly 3 as input, repeat minimap2 + racon => polished assembly 4 Settings - Run as-is or change parameters at runtime. - For the input at \"minimap settings for long reads\", enter (map-pb) for PacBio reads, (map-hifi) for PacBio HiFi reads, or (map-ont) for Oxford Nanopore reads. Outputs There is one output: the polished assembly in fasta format.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome-assembly/polish-with-long-reads/Assembly-polishing-with-long-reads.ga"
  },
  {
    "workflow_id": "iwc_quality-and-contamination-control-raw-reads",
    "category": "genome-assembly",
    "workflow_repository": "quality-and-contamination-control-raw-reads",
    "tool_names": [
      "__ZIP_COLLECTION__",
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.26.0+galaxy0",
      "__UNZIP_COLLECTION__",
      "toolshed.g2.bx.psu.edu/repos/iuc/kraken2/kraken2/2.1.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bracken/est_abundance/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/recentrifuge/recentrifuge/1.16.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator/tooldistillator/1.0.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/tooldistillator_summarize/tooldistillator_summarize/1.0.0+galaxy1"
    ],
    "readme_cleaned": "Quality and Contamination control workflow for paired end data This workflow uses paired-end illumina fastq(.gz) files and executes the following steps: 1. Quality control and trimming - fastp QC control and trimming 2. Taxonomic assignation on trimmed data - Kraken2 assignation - Bracken to re-estimate abundance to the species level - Recentrifuge to make a krona chart 3. Aggregating outputs into a single JSON file - ToolDistillator to extract and aggregate information from different tool outputs to JSON parsable files Inputs 1. Paired-end illumina raw reads in fastq(.gz) format. Outputs 1. Quality control: - quality report - trimmed raw reads 2. Taxonomic assignation: - Tabular report of identified species - Tabular file with assigned read to a taxonomic level - Krona chart to illustrate species diversity of the sample 3. Aggregating outputs: - JSON file with information about the outputs of fastp, Kraken2, Bracken, Recentrifuge",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome-assembly/quality-and-contamination-control-raw-reads/quality_and_contamination_control_raw_reads.ga"
  },
  {
    "workflow_id": "iwc_annotation-helixer",
    "category": "genome_annotation",
    "workflow_repository": "annotation-helixer",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/genouest/helixer/helixer/0.3.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.8.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/jcvi_gff_stats/jcvi_gff_stats/0.8.4",
      "toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/omark/omark/0.3.1+galaxy1"
    ],
    "readme_cleaned": "Genome annotation workflow with Helixer This workflow annotates a genome using Helixer and evaluates the quality of the annotation with BUSCO and Genome Annotation Statistics. GFFRead is used to extract predicted protein sequences, and both BUSCO and OMArk assess proteome quality. The final annotation can be visualized interactively using JBrowse. Helixer is an annotation software with a new and different approach: it performs evidence-free predictions (no need for RNASeq data or sequence aligments), using Graphics Processing Unit (GPU), with a much faster execution time. The annotation is based on the development and use of a cross-species deep learning model. The software is used to configure and train models for ab initio prediction of gene structure. In other words, it identifies the base pairs in a genome that belong to the UTR/CDS/Intron genes. Workflow steps - Genome annotation with Helixer - Extraction of predicted proteins from annotation with GFFRead - Evaluation of annotation - Genome Annotation Statistics - BUSCO (on genome and predicted proteins) - Proteome quality assessment with OMArk - Visualization of annotation with JBrowse Input data The ollowing input files are required for the workflow: - Genome sequence (FASTA format): The genome to be annotated. Used by Helixer, Genome Annotation Statistics, BUSCO, GFFRead, and JBrowse. Output data The workflow generates the following outputs: - Annotation file (GFF3): Contains the final consensus gene models produced by Helixer. - BUSCO results: Assess the completeness of the annotation and include: - A summary of results. - A table of all searched BUSCO genes with their status. - A table of missing BUSCO genes. - Annotation statistics: Summary and graphical analyses of the annotation, produced by Genome Annotation Statistics. - Protein sequences (FASTA): Predicted from the annotation using GFFRead. - OMArk report: completeness, consistency, and contamination of the predicted proteome. - Genome browser visualization (HTML): An interactive genome view produced by JBrowse.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome_annotation/annotation-helixer/Galaxy-Workflow-annotation_helixer.ga"
  },
  {
    "workflow_id": "iwc_annotation-maker",
    "category": "genome_annotation",
    "workflow_repository": "annotation-maker",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fasta_stats/fasta-stats/2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/busco/busco/5.7.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/maker/maker/2.31.11+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/jcvi_gff_stats/jcvi_gff_stats/0.8.4",
      "toolshed.g2.bx.psu.edu/repos/iuc/maker_map_ids/maker_map_ids/2.31.11",
      "toolshed.g2.bx.psu.edu/repos/iuc/jbrowse/jbrowse/1.16.11+galaxy1"
    ],
    "readme_cleaned": "Genome annotation workflow with Maker This workflow allows for genome annotation using Maker and evaluates the quality of the annotation with BUSCO and genome annotation statistics. The annotation can then be improved, standardized, and visualized with additional tools. Maker is a genome model prediction software that uses ab initio predictors (SANP and Augustus) to improve its predictions. Maker is capable of annotating both prokaryotes and eukaryotes. It works by aligning as much evidence as possible along the genome sequence, then reconciling all these signals to determine likely genetic structures. Workflow Steps - Annotation with Maker: Maker uses the genome sequence, protein evidence, ab-initio predictions, and ESTs to produce the annotation. - Quality Evaluation: - Run Fasta Statistics to assess genome assembly quality. - Use BUSCO to evaluate annotation completeness. - Annotation Statistics: Analyze the annotation using Genome Annotation Statistics, producing graphical and textual summaries. - Sequence Extraction: Extract predicted protein sequences using GFFRead for downstream analysis. - Improve Gene Names: Standardize gene names using Map annotation ids for better readability. - Visualization: Load the genome sequence and annotation into JBrowse for interactive browsing. Input data The following input files are required for the workflow: - Genome sequence (FASTA format): The genome to be annotated. Used by Maker, Fasta Statistics, and BUSCO. - Protein sequences (FASTA format): Evidence to assist annotation in Maker. - EST evidences (FASTA format): Alignments used as evidence by Maker. - Ab-initio gene predictions: Supplementary data for Maker to refine annotations. Output Data The workflow generates the following outputs: - Annotation file (GFF3): Contains the final consensus gene models produced by Maker. - Genome statistics: A tabular file summarizing contig sizes and base content, produced by Fasta Statistics. - BUSCO results: Assess the completeness of the annotation and include: - A summary of results. - A table of all searched BUSCO genes with their status. - A table of missing BUSCO genes. - Annotation statistics: Summary and graphical analyses of the annotation, produced by Genome Annotation Statistics. - Protein sequences (FASTA): Predicted from the annotation using GFFRead. - Renamed GFF annotation file: Contains standardized gene names, produced by Map annotation ids. - Genome browser visualization (HTML): An interactive genome view produced by JBrowse.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome_annotation/annotation-maker/Genome_annotation_with_maker_short.ga"
  },
  {
    "workflow_id": "iwc_lncrnas-annotation",
    "category": "genome_annotation",
    "workflow_repository": "lncrnas-annotation",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/gffread/gffread/2.2.1.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/feelnc/feelnc/0.2.1+galaxy0",
      "cat1"
    ],
    "readme_cleaned": "lncRNAs annotation workflow This workflow uses the FEELnc tool to annotate long non-coding RNAs. Before annotating these long non-coding RNAs, StringTie will be used to assemble the RNA-seq alignments into potential trancriptions. The gffread tool provides a genome annotation file in GTF format. For future analyses, it would be interesting to use an updated annotation containing messenger RNA and long non-coding RNA. The concatenante tool merges the reference annotation with the long non-coding RNA annotation obtained with FEELnc. This workflow is taken from the tutorial “Long non-coding RNAs (lncRNAs) annotation with FEELnc” on the GTN. Workflows steps - Transcript Assembly with StringTie: RNA-seq alignments are assembled into potential transcripts to provide a comprehensive view of expressed regions. - Genome Annotation Conversion with GFFRead: Genome annotations are converted into a standardized format (GTF) to ensure compatibility with downstream tools. - lncRNA Annotation with FEELnc: The FEELnc pipeline identifies and classifies long non-coding RNAs (lncRNAs) through three main steps: - Filter: Removes unwanted transcripts and those overlapping reference exons. - Codpot: Evaluates coding potential to differentiate lncRNAs from coding RNAs. - Classifier: Assigns lncRNAs to categories based on their genomic location and transcriptional direction. - Annotation Merging with Concatenate: The lncRNA annotation is merged with the reference annotation to create a unified genome annotation containing both mRNAs and lncRNAs. Input data The following input files are required for the workflow: - RNA-seq alignments (BAM format): Required by StringTie for transcript assembly. - Genome annotation (GFF3 format): Used by StringTie and GFFRead for processing. - Genome sequence (FASTA format): Required by FEELnc for lncRNA identification. - Reference annotation (GTF format): Provided by GFFRead for FEELnc analysis. Output data The workflow produces the following outputs: - Transcript annotation (GTF format): Generated by StringTie, containing assembled transcripts from RNA-seq data. - Converted genome annotation (GTF format): Produced by GFFRead, used as input for FEELnc. - lncRNA annotation (GTF format): Generated by FEELnc, containing identified lncRNAs. - mRNA annotation (GTF format): Produced by FEELnc for downstream use. - lncRNA classification table: Produced by FEELnc, detailing genomic relationships of lncRNAs. - Comprehensive genome annotation (GTF format): Generated by Concatenate, combining mRNA and lncRNA annotations.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/genome_annotation/lncRNAs-annotation/Galaxy-Workflow-lncRNAs_annotation_workflow.ga"
  },
  {
    "workflow_id": "iwc_fluorescence-nuclei-segmentation-and-counting",
    "category": "imaging",
    "workflow_repository": "fluorescence-nuclei-segmentation-and-counting",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/imgteam/2d_simple_filter/ip_filter_standard/1.12.0+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/imgteam/2d_histogram_equalization/ip_histogram_equalization/0.18.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/imgteam/2d_auto_threshold/ip_threshold/0.18.1+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/imgteam/bfconvert/ip_convertimage/6.7.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/imgteam/binary2labelimage/ip_binary_to_labelimage/0.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/imgteam/overlay_images/ip_overlay_images/0.0.4+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/imgteam/count_objects/ip_count_objects/0.0.5-2"
    ],
    "readme_cleaned": "Segmentation and counting of cell nuclei in fluorescence microscopy images This workflow performs segmentation and counting of cell nuclei using fluorescence microscopy images. The segmentation step is performed using Otsu thresholding (Otsu, 1979). The workflow is based on the tutorial: https://training.galaxyproject.org/training-material/topics/imaging/tutorials/imaging-introduction/tutorial.html ![](test-data/overlayimage.png) Inputs inputimage: The fluorescence microscopy images to be segmented. Must be the single image channel, which contains the cell nuclei. Outputs overlayimage: An overlay of the original image and the outlines of the segmentated objects, each also annotated with a unique number. objectscount: Table with a single column objects and a single row (the actual number of objects). labelimage: The segmentation result (label map, which contains a unique label for each segmented object).",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/imaging/fluorescence-nuclei-segmentation-and-counting/segmentation-and-counting.ga"
  },
  {
    "workflow_id": "iwc_gcms-metams",
    "category": "metabolomics",
    "workflow_repository": "gcms-metams",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lecorguille/msnbase_readmsdata/msnbase_readmsdata/2.16.1+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_xcmsset/abims_xcms_xcmsSet/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_plot_chromatogram/xcms_plot_chromatogram/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_merge/xcms_merge/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/yguitton/metams_rungc/metams_runGC/3.0.0+metaMS1.24.0-galaxy0",
      "toolshed.g2.bx.psu.edu/repos/ethevenot/checkformat/checkFormat/3.0.0",
      "toolshed.g2.bx.psu.edu/repos/ethevenot/multivariate/Multivariate/2.3.10"
    ],
    "readme_cleaned": "Mass spectrometry: GCMS with metaMS This workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments. This workflow is composed with the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract and the metaMS R package [(Wehrens, R 2014)](https://bioconductor.org/packages/release/bioc/html/metaMS.html) for the field of untargeted metabolomics. 🎓 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: GC-MS analysis with metaMS package](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/gcms/tutorial.html) Inputs sampleMetadata The sampleMetadata tabular file corresponds to a table containing information about your samples A sample metadata file contains various information for each of your raw files: - Classes which will be used during the preprocessing steps - Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order - Different experimental conditions which can be used for statistics - Any information about samples that you want to keep, in a column format The content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either: - Upload an existing metadata file - Use a template to create one (because it can be painful to get the sample list without misspelling or omission) - Generate a template with the xcms get a sampleMetadata file tool available in Galaxy - Fill it using your favorite table editor (Excel, LibreOffice) - Upload it within Galaxy Formats: tab-separated values as tsv, tab, txt, ... Mass-spectrometry Dataset Collection Mass-spectrometry data files gathered in a Galaxy Dataser Collection Formats: open format as mzXML, mzMl, mzData and netCDF Main steps 1. MSnbase readMSData: read the mzXML and prepare for xcms 2. XCMS findChromPeaks: peak picking 3. metaMS.runGC: definition of pseudo-spectra",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/metabolomics/gcms-metams/Mass-spectrometry__GCMS-with-metaMS.ga"
  },
  {
    "workflow_id": "iwc_lcms-preprocessing",
    "category": "metabolomics",
    "workflow_repository": "lcms-preprocessing",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/lecorguille/msnbase_readmsdata/msnbase_readmsdata/2.16.1+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_plot_chromatogram/xcms_plot_chromatogram/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_xcmsset/abims_xcms_xcmsSet/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_merge/xcms_merge/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_group/abims_xcms_group/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_retcor/abims_xcms_retcor/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/ethevenot/checkformat/checkFormat/3.0.0",
      "toolshed.g2.bx.psu.edu/repos/melpetera/intensity_checks/intens_check/1.3.0",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/xcms_fillpeaks/abims_xcms_fillPeaks/3.12.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/lecorguille/camera_annotate/abims_CAMERA_annotateDiffreport/2.2.7+camera1.48.0-galaxy1"
    ],
    "readme_cleaned": "Mass spectrometry: LC-MS preprocessing with XCMS This workflow uses the XCMS tool R package [(Smith, C.A. 2006)](https://bioconductor.org/packages/release/bioc/html/xcms.html) to extract, filter, align and fill gaps, and uses the CAMERA R package [(Kuhl, C 2012)](https://bioconductor.org/packages/release/bioc/html/CAMERA.html) to annotate isotopes, adducts and fragments. 🎓 For more information see the [Galaxy Training Network tutorial: Mass spectrometry: LC-MS preprocessing with XCMS](https://training.galaxyproject.org/training-material/topics/metabolomics/tutorials/lcms-preprocessing/tutorial.html) Inputs sampleMetadata The sampleMetadata tabular file corresponds to a table containing information about your samples A sample metadata file contains various information for each of your raw files: - Classes which will be used during the preprocessing steps - Analytical batches which will be useful for a batch correction step, along with sample types (pool/sample) and injection order - Different experimental conditions which can be used for statistics - Any information about samples that you want to keep, in a column format The content of your sample metadata file has to be filled by you, since it is not contained in your raw data. Note that you can either: - Upload an existing metadata file - Use a template to create one (because it can be painful to get the sample list without misspelling or omission) - Generate a template with the xcms get a sampleMetadata file tool available in Galaxy - Fill it using your favorite table editor (Excel, LibreOffice) - Upload it within Galaxy Formats: tab-separated values as tsv, tab, txt, ... Mass-spectrometry Dataset Collection Mass-spectrometry data files gathered in a Galaxy Dataser Collection Formats: open format as mzXML, mzMl, mzData and netCDF Main steps 1. MSnbase readMSData: read the mzXML and prepare for xcms 2. XCMS findChromPeaks: peak picking 3. XCMS groupChromPeaks: determining shared ions across samples 4. XCMS adjustRtime: retention time correction 5. XCMS fillChromPeaks: integrating areas of missing peaks 6. CAMERA.annotate: annotation",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/metabolomics/lcms-preprocessing/Mass_spectrometry__LC-MS_preprocessing_with_XCMS.ga"
  },
  {
    "workflow_id": "iwc_mfassignr",
    "category": "metabolomics",
    "workflow_repository": "mfassignr",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_kmdnoise/mfassignr_kmdnoise/1.1.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_histnoise/mfassignr_histnoise/1.1.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_snplot/mfassignr_snplot/1.1.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_isofiltr/mfassignr_isofiltr/1.1.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassigncho/mfassignr_mfassignCHO/1.1.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recallist/mfassignr_recallist/1.1.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_findrecalseries/mfassignr_findRecalSeries/1.1.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_recal/mfassignr_recal/1.1.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/recetox/mfassignr_mfassign/mfassignr_mfassign/1.1.2+galaxy1"
    ],
    "readme_cleaned": "Molecular Formula Assignment and Recalibration Workflow with MFAssignR This workflow is designed for molecular formula assignment and recalibration of mass spectrometry data using the MFAssignR tool. It processes feature tables to generate recalibrated series, molecular formula assignments, and various diagnostic plots. Workflow Steps 1. Input Feature Table: - Accepts a feature table in tabular format containing mass spectrometry data. 2. Molecular Formula Assignment: - Assigns molecular formulas to features based on mass-to-charge ratios and isotopic patterns. 3. Recalibration: - Recalibrates mass spectrometry data to improve accuracy. 4. Visualization: - Generates diagnostic plots, including: - Signal-to-noise (SN) plots. - Mass-to-charge (MZ) error plots. - Van Krevelen (VK) diagrams. - Molecular formula assignment plots. Inputs - Feature Table: A tabular file containing mass spectrometry data. Example input: mfassignrinput.txt. Outputs - Recalibrated Series: - recalseries.tabular: Recalibrated data series. - finalseries.tabular: Final recalibrated series. - Molecular Formula Assignments: - Ambig.tabular: Ambiguous assignments. - Unambig.tabular: Unambiguous assignments. - Diagnostic Plots: - Signal-to-noise plot: SNplot.png. - Mass-to-charge error plot: MZplot.png. - Van Krevelen diagrams and molecular formula assignment plots for CHO and other elements. Usage This workflow is designed to be run on the Galaxy platform. Users can upload their feature table, configure parameters, and execute the workflow to obtain recalibrated data, molecular formula assignments, and diagnostic plots. References - [MFAssignR Documentation](https://github.com/your-repo/mfassignr) License This workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/metabolomics/mfassignr/mfassignr.ga"
  },
  {
    "workflow_id": "iwc_qcxms-sdf",
    "category": "metabolomics",
    "workflow_repository": "qcxms-sdf",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/openbabel_compound_convert/openbabel_compound_convert/3.1.1+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_neutral_run/qcxms_neutral_run/5.2.1+galaxy6",
      "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_production_run/qcxms_production_run/5.2.1+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/recetox/qcxms_getres/qcxms_getres/5.2.1+galaxy3"
    ],
    "readme_cleaned": "QCxMS Spectra Prediction from SDF Workflow This workflow predicts electron ionization (EI) mass spectra using QCxMS, starting from a single SDF file containing the 3D coordinates of all atoms in the molecule. These files can typically be obtained from PubChem. The workflow converts the input file, performs neutral and production runs, and generates predicted spectra in MSP format. Workflow Steps 1. Input SDF File: - Accepts an SDF file containing one or multiple molecular structures with pre-generated conformers. 2. Conversion to XYZ Format: - Converts the input SDF file to XYZ format using Open Babel. 3. QCxMS Neutral Run: - Performs a neutral run to prepare the molecular structure for production calculations. 4. QCxMS Production Run: - Executes the production run to simulate fragmentation and generate intermediate results. 5. QCxMS Get Results: - Processes the results from the production run and generates the predicted EI mass spectra in MSP format. Inputs - Input SDF File: A file containing molecular structures with 3D coordinates (e.g., obtained from PubChem). Outputs - Predicted Spectra: - An MSP file containing the predicted EI mass spectra for the input molecules. Usage This workflow is designed to be run on the Galaxy platform. Users can upload their SDF file, configure parameters, and execute the workflow to obtain predicted EI mass spectra. References - [QCxMS Documentation](https://github.com/recetox/qcxms) - [Open Babel Documentation](http://openbabel.org/) License This workflow is distributed under the MIT License. Please ensure proper attribution when using or modifying this workflow.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/metabolomics/qcxms-sdf/QCxMS-Spectra-Prediction-from-SDF.ga"
  },
  {
    "workflow_id": "iwc_mags-building",
    "category": "microbiome",
    "workflow_repository": "mags-building",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "__UNZIP_COLLECTION__",
      "toolshed.g2.bx.psu.edu/repos/iuc/megahit/megahit/1.2.9+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/nml/metaspades/metaspades/4.2.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/quast/quast/5.3.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/concoct_cut_up_fasta/concoct_cut_up_fasta/1.1.0+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_sort/samtools_sort/2.0.5",
      "toolshed.g2.bx.psu.edu/repos/iuc/semibin/semibin/2.0.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/concoct_coverage_table/concoct_coverage_table/1.1.0+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/metabat2_jgi_summarize_bam_contig_depths/metabat2_jgi_summarize_bam_contig_depths/2.17+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/fasta_to_contig2bin/Fasta_to_Contig2Bin/1.1.7+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/concoct/concoct/1.1.0+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/mbernt/maxbin2/maxbin2/2.2.7+galaxy6",
      "toolshed.g2.bx.psu.edu/repos/iuc/metabat2/metabat2/2.17+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/concoct_merge_cut_up_clustering/concoct_merge_cut_up_clustering/1.1.0+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/concoct_extract_fasta_bins/concoct_extract_fasta_bins/1.1.0+galaxy2",
      "__BUILD_LIST__",
      "toolshed.g2.bx.psu.edu/repos/iuc/binette/binette/1.1.1+galaxy0",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/checkm2/checkm2/1.0.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/drep_dereplicate/drep_dereplicate/3.6.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/gtdbtk_classify_wf/gtdbtk_classify_wf/2.4.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/checkm_lineage_wf/checkm_lineage_wf/1.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/coverm_genome/coverm_genome/0.7.0+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bakta/bakta/1.9.4+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/collection_column_join/collection_column_join/0.0.3",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3"
    ],
    "readme_cleaned": "Metagenome-Assembled Genomes (MAGs) Generation This workflow generates Metagenome-Assembled Genomes (MAGs) from paired short reads. Dereplicated MAGs for the complete input sample set are reported. Workflow Logic The workflow supports assembly using metaSPADES and MEGAHIT. For binning, it utilizes four different tools: MetaBAT2, MaxBin2, SemiBin, and CONCOCT. The resulting bins are then refined using Binette, the successor of metaWRAP. MAGs Annotation and Quality Control After binning, the resulting MAGs are dereplicated across all input samples based on CheckM2 quality metrics using dRep. The following processing steps are then performed: - Annotation with Bakta - Taxonomic Assignment using GTDB-Tk - Quality Control via QUAST and CheckM/CheckM2 - Abundance Estimation per sample with CoverM All results are consolidated into a single MultiQC report for easy analysis. Input Requirements Input reads must be quality-filtered, with host reads removed. - Trimmed reads: Quality-trimmed reads from individual samples, used solely for abundance estimation. - Trimmed reads from grouped samples: These reads need to be grouped based on the desired MAGs generation approach: - Individual MAGs Generation: Use the same input as Sample-wise Trimmed Paired Reads to generate MAGs per sample. - Pooled MAGs Generation (Co-assembly/Binning): Merge all reads input one file for a fully pooled MAGs approach. - Grouped MAGs Generation (Co-assembly/Binning): Merge samples based on predefined groups. - Hybrid MAGs Generation: Combine individual and grouped reads for a mixed approach. > Note: Merging reads can result in large input files, significantly increasing computational demands—especially during assembly and binning, which may require substantial RAM. Our tests with synthetic samples up to 50 GB showed feasible performance. For larger datasets, we recommend limiting the approach to individual or pooled MAGs generation.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/microbiome/mags-building/MAGs-generation.ga"
  },
  {
    "workflow_id": "iwc_openms-metaprosip",
    "category": "proteomics",
    "workflow_repository": "openms-metaprosip",
    "tool_names": [
      "__SORTLIST__",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_decoydatabase/DecoyDatabase/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_featurefindermultiplex/FeatureFinderMultiplex/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_msgfplusadapter/MSGFPlusAdapter/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_peptideindexer/PeptideIndexer/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_falsediscoveryrate/FalseDiscoveryRate/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_idmapper/IDMapper/3.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/galaxyp/openms_metaprosip/MetaProSIP/3.1+galaxy0"
    ],
    "readme_cleaned": "MetaProSIP: automated inference of elemental fluxes in microbial communities Inputs dataset - Centroided LC-MS datasets in mzML (MetaProSIP is mainly tested on data generated by orbitrap instruments) - Fasta Database in Fasta (aminoacid sequences) Inputs values - Precursor monoisotopic mass tolerance (ppm): This value is passed to - MSGFPlusAdapter parameter Precursor monoisotopic mass tolerance (-precursormasstolerance) - MetaProSIP parameter Tolerance in ppm (-mztoleranceppm) - Fixed modifications - Variable modifications - Labeled element Processing - DecoyDatabase: Add decoy sequences to the Fasta database (for FDR calculation) - FeatureFinderCentroided: identify eluting peptides that correspond to isotopologues with natural isotopic distributions - MSGFPlusAdapter: identify peptides through peptide fragment fingerprinting (database search) - FeatureFinderMultiplex: detect elution profiles of unlabeled peptides - PeptideIndexer: annotate protein association to identified peptides - FalseDiscoveryRate: Calculate FDR - IDMapper: map identified spectra to elution profiles - MetaProSIP: calculate the protein-SIP features, to perform functional grouping, and for protein inference",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/proteomics/openms-metaprosip/metaprosip.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-consensus-from-variation",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-consensus-from-variation",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_genomecoveragebed/2.31.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_filter/4.3+t.galaxy1",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/concat/gops_concat_1/1.0.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/merge/gops_merge_1/1.0.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/subtract/gops_subtract_1/1.0.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/bcftools_consensus/bcftools_consensus/1.15.1+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0"
    ],
    "readme_cleaned": "COVID-19: consensus construction -------------------------------- This workflow aims at generating reliable consensus sequences from variant calls according to transparent criteria that capture at least some of the complexity of variant calling. It takes a collection of VCFs (with DP and DP4 INFO fields) and a collection of the corresponding aligned reads (for the purpose of calculating genome-wide coverage) such as produced by any of the variant calling workflows in https://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling and generates a collection of viral consensus sequences and a multisample FASTA of all these sequences. Each consensus sequence is guaranteed to capture all called, filter-passing (as per the FILTER column of the VCF input) variants found in the VCF of its sample that reach a user-defined consensus allele frequency threshold. Filter-failing variants and variants below a second user-defined minimal allele frequency threshold will be ignored. Genomic positions of filter-passing variants with an allele frequency in between the two thresholds will be hard-masked (with N) in the consensus sequence of their sample. Genomic positions with a coverage (calculated from the read alignments input) below another user-defined threshold will be hard-masked, too, unless they are consensus variant sites.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-consensus-from-variation/consensus-from-variation.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-ont-artic-variant-calling",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-ont-artic-variant-calling",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.20.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/1.6",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/1.1.3",
      "toolshed.g2.bx.psu.edu/repos/iuc/minimap2/minimap2/2.17+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.9+galaxy2",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.2+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/freebayes/bamleftalign/1.3.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.3.1+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.2.2d+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/medaka_consensus/medaka_consensus/1.0.3+galaxy2",
      "__FILTER_FAILED_DATASETS__",
      "toolshed.g2.bx.psu.edu/repos/iuc/medaka_variant/medaka_variant/1.3.2+galaxy1",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/bedtools/bedtools_intersectbed/2.30.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.8+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.10",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/1.1.3"
    ],
    "readme_cleaned": "COVID-19: variation analysis on ARTIC ONT data ---------------------------------------------- This workflow for ONT-sequenced ARTIC data is modeled after the alignment/variant-calling steps of the [ARTIC pipeline](https://artic.readthedocs.io/en/latest/). It performs, essentially, the same steps as that pipeline’s minion command, i.e. read mapping with minimap2 and variant calling with medaka. Like the Illumina ARTIC workflow it uses ivar for primer trimming. Since ONT-sequenced reads have a much higher error rate than Illumina-sequenced reads and are therefor plagued more by false-positive variant calls, this workflow does make no attempt to handle amplicons affected by potential primer-binding site mutations.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-ont-artic-variant-calling/ont-artic-variation.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-pe-illumina-artic-ivar-analysis",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-pe-illumina-artic-ivar-analysis",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.23.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/1.1.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.4",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.15.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.2.2d+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.4.2+galaxy0",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_consensus/ivar_consensus/1.4.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.11+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/pangolin/pangolin/4.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/nextclade/nextclade/2.7.0+galaxy0"
    ],
    "readme_cleaned": "COVID-19 sequence analysis on Illumina Amplicon PE data This workflow implements an [iVar](https://github.com/andersen-lab/ivar) based analysis similar to the one in [ncov2019-artic-nf](https://github.com/connor-lab/ncov2019-artic-nf), [covid-19-signal](https://github.com/jaleezyy/covid-19-signal/) and the Thiagen [Titan workflow](https://github.com/theiagen/publichealthviralgenomics). These workflows (written in Nextflow, Snakemake and WDL) are widely in use in [COG UK](https://www.cogconsortium.uk/), [CanCOGeN](https://www.genomecanada.ca/en/cancogen) and some US state public health laboratories. This workflow is also the subject of a Galaxy Training Network tutorial (currently a [Work in Progress](https://github.com/galaxyproject/training-material/pull/2633)). It differs from [this workflow](https://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-artic-variant-calling) in that it does not use lofreq and is aimed at rapid analysis of majority variants and lineage/clade assignment with pangolin and nextclade. TODO: 1. Add support for QC using negative and positive controls 2. Integrate with phylogeny tools including IQTree and UShER (and possibly more).",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-artic-ivar-analysis/pe-wgs-ivar-analysis.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-pe-illumina-artic-variant-calling",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-pe-illumina-artic-variant-calling",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.18",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.5",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.4.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_filter/4.3+t.galaxy1",
      "__FILTER_FAILED_DATASETS__",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_removereads/ivar_removereads/1.4.4+galaxy0",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/bcftools_annotate/bcftools_annotate/1.15.1+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/devteam/vcfvcfintersect/vcfvcfintersect/1.0.0_rc3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0"
    ],
    "readme_cleaned": "COVID-19: variation analysis on ARTIC PE data --------------------------------------------- The workflow for Illumina-sequenced ampliconic data builds on the RNASeq workflow for paired-end data using the same steps for mapping and variant calling, but adds extra logic for trimming amplicon primer sequences off reads with the ivar package. In addition, this workflow uses ivar also to identify amplicons affected by primer-binding site mutations and, if possible, excludes reads derived from such \"tainted\" amplicons when calculating allele-frequencies of other variants.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-artic-variant-calling/pe-artic-variation.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-pe-illumina-wgs-variant-calling",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-pe-illumina-wgs-variant-calling",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.23.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.13+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.3",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/2.18.2.3",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19"
    ],
    "readme_cleaned": "COVID-19: variation analysis on WGS PE data ------------------------------------------- This workflows performs paired end read mapping with bwa-mem followed by sensitive variant calling across a wide range of AFs with lofreq and variant annotation with snpEff 4.5covid19.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-pe-illumina-wgs-variant-calling/pe-wgs-variation.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-se-illumina-wgs-variant-calling",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-se-illumina-wgs-variant-calling",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy4",
      "toolshed.g2.bx.psu.edu/repos/devteam/bowtie2/bowtie2/2.5.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/3.1.1.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff_sars_cov_2/snpeff_sars_cov_2/4.5covid19"
    ],
    "readme_cleaned": "COVID-19: variation analysis on WGS SE data ------------------------------------------- This workflows performs single end read mapping with bowtie2 followed by sensitive variant calling across a wide range of AFs with lofreq and variant annotation with snpEff 4.5covid19.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-se-illumina-wgs-variant-calling/se-wgs-variation.ga"
  },
  {
    "workflow_id": "iwc_sars-cov-2-variation-reporting",
    "category": "sars-cov-2-variant-calling",
    "workflow_repository": "sars-cov-2-variation-reporting",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_filter/4.3+t.galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/replace_column_by_key_value_file/replace_column_with_key_value_file/0.2",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/2.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.8+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.3+galaxy1",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_header_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpfreqplot/snpfreqplot/1.0+galaxy3"
    ],
    "readme_cleaned": "COVID-19: variation analysis reporting -------------------------------------- This workflow takes VCF datasets of variants produced by any of the \"-variant-calling\" workflows in https://github.com/galaxyproject/iwc/tree/main/workflows/sars-cov-2-variant-calling and generates tabular reports of variants by samples and by variant, along with an overview plot of variants and their allele-frequencies across all samples.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/sars-cov-2-variant-calling/sars-cov-2-variation-reporting/variation-reporting.ga"
  },
  {
    "workflow_id": "iwc_baredsc",
    "category": "scrnaseq",
    "workflow_repository": "baredsc",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/baredsc_1d/baredsc_1d/1.1.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/baredsc_combine_1d/baredsc_combine_1d/1.1.3+galaxy0"
    ],
    "readme_cleaned": "Single-Cell Mixture Analysis: baredSC Log-Normalized Models These workflows allow to run a baredSC analysis from a table with counts in a single click. It uses models from 1 to N Gaussians and combine them. It uses the logNorm scale, 100 bins for 1 dimension and 25 bins on each axis in 2 dimensions. Inputs dataset - Both workflows need a tabular dataset where each row is a cell. The tabular needs to have a header line with column names. There must be at least two columns: 'nCountRNA' and another one with the counts for the gene(s) of interest. A way to get such table in R from a Seurat object (seurat.obj) is: r my.genes <- c(\"Hoxa13\", \"Hoxd13\") df <- cbind(seurat.obj[[]], This will give you all metadata including nCountRNA FetchData(seurat.obj, slot = \"counts\", vars = my.genes)) write.table(df, \"inputforbaredSC.txt\", quote = F, sep = \"\\t\", row.names = F) Inputs values For the 1D: - Gene name: The name of the column with the counts of your gene of interest. - Maximum value in logNorm: The maximum value to explore in PDF. This value should be large enough so the PDF is at 0 at this value. - Maximum number of Gaussians to study: All models between models with 1 Gaussians to models with this number of Gaussians will be combined. For the 2D: - Gene name for x axis: The name of the column with the counts of your gene in x axis. - Gene name for y axis: The name of the column with the counts of your gene in y axis. - maximum value in logNorm for x-axis: The maximum value to explore in PDF in the x axis. This value should be large enough so the PDF is at 0 at this value. - maximum value in logNorm for y-axis: The maximum value to explore in PDF in the y axis. This value should be large enough so the PDF is at 0 at this value. - Maximum number of Gaussians to study: All models between models with 1 2D-Gaussians to models with this number of 2D-Gaussians will be combined. - compute p-value: Whether you want to get a p-value. As a consequence, less samples than available will be used for plots as p-value computation requires to have independent samples. Processing - The workflow will generate paramater values from 1 to the maximum number of Gaussians to study. - baredSC1d or baredSC2d is run for each of these number of Gaussians - All models are combined into a single result.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/scRNAseq/baredsc/baredSC-1d-logNorm.ga"
  },
  {
    "workflow_id": "iwc_fastq-to-matrix-10x",
    "category": "scrnaseq",
    "workflow_repository": "fastq-to-matrix-10x",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/cite_seq_count/cite_seq_count/1.4.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.5+galaxy2"
    ],
    "readme_cleaned": "Single-Cell RNA-seq Preprocessing: 10X Genomics Data to Analysis-Ready Formats These workflows are inspired by the [training material](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-preprocessing-tenx/tutorial.html). The output is in a 'bundle' format with three files (one matrix, one with genes, one with barcodes) which is similar to the cellranger output format, and is compatible with any Read10X function (Seurat or Scanpy). There are 2 types of output collections: either one collection with all matrices, one collection with all genes and one collection with all barcodes (compatible with post processing in galaxy) or a single nested collection with one sub-collection per sample with the 3 files (easier for local downstream analysis). Both workflows are designed for fastqs from 10X libraries v3. One is for regular 10X library (one library per sample), while the other one is for CellPlex 10X library which allows to multiplex samples using CMOs (see [this blog article](https://www.10xgenomics.com/blog/answering-your-questions-about-sample-multiplexing-for-single-cell-gene-expression)). Input datasets - Specific for each experiment: - For both workflows: you need a list of pairs of fastqs with gene expression. - For CellPlex: you need in addition a list of pairs of fastqs with CMO. - For CellPlex: you need a list of csv which describes samples and CMO used: - first column is the sequence and second column is the name /!\\ The order of samples need to be exactly the same between the collection of fastqs of CMO and the collection of csv. - Common for all experiments: - Gene annotations: A gtf file with gene locations - List of barcodes used by 10X. You can download it at https://zenodo.org/record/3457880/files/3M-february-2018.txt.gz Input values - reference genome: this genome needs to be available for STAR - Barcode Size is same size of the Read: if the length of your R1 of GEX matches the size of cell barcode + UMI set to true. If your R1 contains trailling A, put false. - number of cells: If you make it too large no cell barcode correction will be performed to demultiplex CMOs. Processing - Gene expression processing: - Reads are aligned to the genome, asigned to genes, cell barcode and UMI with STAR Solo - MultiQC report the mapping rate and the number of reads attributed to genes - The output of STAR Solo is filtered with Droplet Utils to remove cellular barcodes which are probably empty. - The output of Droplet Utils is reorganized to be: Main Collection: - Sample 1: - matrix.mtx - barcodes.tsv - genes.tsv - Sample 2: - matrix.mtx - barcodes.tsv - genes.tsv ... For the CellPlex workflow: - CMO processing: - CITE-Seq Count is used to asign reads and generate a matrix where 'genes' are the CMO and 'unmapped'. - Cellular barcodes are translated to match the cellular barcodes of Gene expression see [this article](https://kb.10xgenomics.com/hc/en-us/articles/360031133451-Why-is-there-a-discrepancy-in-the-3M-february-2018-txt-barcode-whitelist-). - Reorganize the output with UMI matrices to match the same structure as gene expression matrices. Test data The test dataset has been produced to make it as small as possible in order to make the workflow pass on CI. - The CMO reads come from [zenodo](https://zenodo.org/records/10229382) and have been sampled to 0.1 with seqtk. - The GEX reads come from SRR13948489 but have been subsetted to the cells selected in the above zenodo.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/scRNAseq/fastq-to-matrix-10x/scrna-seq-fastq-to-matrix-10x-cellplex.ga"
  },
  {
    "workflow_id": "iwc_pseudobulk-worflow-decoupler-edger",
    "category": "scrnaseq",
    "workflow_repository": "pseudobulk-worflow-decoupler-edger",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/ebi-gxa/decoupler_pseudobulk/decoupler_pseudobulk/1.4.0+galaxy8",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_line/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/column_remove_by_header/column_remove_by_header/1.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_replace_in_column/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/edger/edger/3.36.0+galaxy5",
      "toolshed.g2.bx.psu.edu/repos/iuc/collection_element_identifiers/collection_element_identifiers/0.0.2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.6"
    ],
    "readme_cleaned": "Single-Cell Pseudobulk Differential Expression Analysis with edgeR This workflow uses the decoupler tool in Galaxy to generate pseudobulk counts from an annotated AnnData file obtained from scRNA-seq analysis. Following the pseudobulk step, differential expression genes (DEG) are calculated using the edgeR tool. The workflow also includes data sanitation steps to ensure smooth operation of edgeR and minimizing potential issues. Additionally, a Volcano plot tool is used to visualize the results after the DEG analysis. The workflow deposited here is based on an earlier version of the [Persist-SEQ](https://persist-seq.org/)[^1] Pseudo-bulk scRNA-seq pipeline, of which the latest version is available [here](https://usegalaxy.eu/published/workflow?id=c3a11e1ac1aa8383). In terms of core procedures, the main differences with the IWC workflow are that the Persist-SEQ workflow: [^1]: The PERSIST-SEQ consortium is funded by the Innovative Medicines Initiative (IMI) Joint Undertaking, which receives support from the European Union's Horizon 2020 research and innovation program and EFPIA. - Is more opinionanted on the downstream enrichment analysis for the cancer biology use case. - Enables filtering out of seldomly expressed genes (with a configurable threshold) per contrast after DE calling, which reduces poorly supported highly DE genes and improves the signal for downstream enrichment analysis. Inputs - deCoupler: Source AnnData (h5ad). - Parameter: Pseudobulk: Fields to merge / optional - Parameter: Group by column / has to be given - Parameter: Sample key column / has to be given - Parameter: Name your raw count layer / has to be given - Parameter: Factor Field / has to be given - edgeR: - Sanitzed Count Matrix - Sanitized Factor File - Cleaned Gene Annotations file - Parameter: Formula for linear model / has to be given - Contrast file / has to be given - Volcano Plot: - Input (tabular) file with genesymbol, logFC, Pvalue and FDR columns. Processing Sanitzation steps after decoupler: - Sanitize Matrix and Factors(tabular): finds [ --+^]+ and replace with - - Remove start, end with (tabular): A column that may affect EdgeR and DESeq2. - Sanitize First Factor for leading digits (tabular): Finds ^([0-9])(.+) and replace it with GG\\\\1\\\\2 - Get Contrast labels - Replace text - Split Contrasts - Contrasts as Parameters: Plot title - Select columns for volcano plot using (Remove columns) from DEG edgeR (Table)output. Outputs - Pseudobulkcountmatrix (tabular) - Pseudobulk Plot (png) - Filtered by expression (png) - Table DEG - Results (HTML) File and plots for download within the output as (png) - Volcano plot (PDF)",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/scRNAseq/pseudobulk-worflow-decoupler-edger/pseudo-bulk_edgeR.ga"
  },
  {
    "workflow_id": "iwc_scanpy-clustering",
    "category": "scrnaseq",
    "workflow_repository": "scanpy-clustering",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/anndata_import/anndata_import/0.10.9+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_filter/scanpy_filter/1.10.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/anndata_inspect/anndata_inspect/0.10.9+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/anndata_manipulate/anndata_manipulate/0.10.9+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_inspect/scanpy_inspect/1.10.2+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_plot/scanpy_plot/1.10.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_normalize/scanpy_normalize/1.10.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_remove_confounders/scanpy_remove_confounders/1.10.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/scanpy_cluster_reduce_dimension/scanpy_cluster_reduce_dimension/1.10.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.8+galaxy0",
      "param_value_from_file"
    ],
    "readme_cleaned": "Single-Cell RNA-seq Analysis: Scanpy Preprocessing and Clustering This workflow follows Scanpy legacy workflow [clustering 3k PBMCs](https://scanpy.readthedocs.io/en/stable/tutorials/basics/clustering-2017.html). For more details on concepts and parameters, please refer to the equivalent Galaxy-based [tutorial](https://training.galaxyproject.org/training-material/topics/single-cell/tutorials/scrna-scanpy-pbmc3k/tutorial.html). Inputs Input datasets - This workflow needs 3 files as input - A single-cell count matrix file in Matrix Market Exchange format - A cell barcodes file with a single barcode in each line. The barcodes should correspond to the cells in the matrix file - A genes/features tabular file with gene ids and gene names. - Cell Ranger v2 or earlier version call this file as genes.tsv and contains two columns: - Gene ID (Ensembl gene ID or other identifiers) - Gene Name (common gene name or symbol) - Cell Ranger v3 or later version call this file as features.tsv and contains three columns: - Feature ID (Ensembl gene ID or other identifiers) - Feature Name (common gene name or symbol) - Feature Type (e.g., Gene Expression, Antibody Capture, CRISPR Guide Capture, etc.) Input parameters - The following parameters should be configured according to the data. - Minimum number of cells expressed per gene. The workflow default is 3. - Minimum number of genes expressed per cell. The workflow default is 200. - Maximum number of genes expressed per cell. The workflow default is 2500. - Size of the local neighborhood. Number of neighbours for computing neighborhood graph. The default is 15. - Size of the local neighborhood (aka resolution) in louvain algorithm. The default is 1. Processing - The workflow creates an Anndata object from the given input files. - Quality control performed. Cells are filtered by number of genes expressed, cells with high mitochondial content are removed. - Then counts are normlized and scaled - PCA is used for dimensionality reduction and 50 PCs are computed. Various plots are generated to inspect the PCA and PCA loadings that helps in chodeterminingnumber of PCs to keep for further analysis. - Clustering is performed by computing a neighbourhood graph, and then using louvain algorithm. neighborhood graph is embeded into UMAP and plotted. - Marker genes are identified using Wilcoxon rank sum test. Marker genes expressions are visualized in various plots. - Optionally, louvain clusters can be annotated with cell types based on the marker genes. Outputs - Final output is an Anndata object with annotations of louvain clusters. - Some informative plots from QC to the end results",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/scRNAseq/scanpy-clustering/Preprocessing-and-Clustering-of-single-cell-RNA-seq-data-with-Scanpy.ga"
  },
  {
    "workflow_id": "iwc_velocyto",
    "category": "scrnaseq",
    "workflow_repository": "velocyto",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/velocyto_cli/velocyto_cli/0.17.17+galaxy2"
    ],
    "readme_cleaned": "RNA Velocity Analysis: Velocyto for 10X Genomics Data These workflows simply run velocyto. There are 2 workflows because one can be easily run after the Single-Cell RNA-seq Preprocessing: 10X Genomics v3 to Seurat-Compatible Format' workflows (RNA Velocity Analysis: Velocyto for 10X Data from Bundled Output). The other can be easily run from uploaded datasets (RNA Velocity Analysis: Velocyto for 10X Data with Filtered Barcodes). Input datasets - BAM files with CB and UB: A collection of BAM. It accepts BAM from cellranger or STARsolo with the CB and UB tags (if you use the fastq-to-matrix-10x workflows these tags are automatically included). - filtered barcodes (only for Velocytoon10Xfilteredbarcodes workflow): A collection of filtered barcodes (this is what will be used by velocyto). 'Filtered' means that these barcodes have been identified as potential cells. It should not be the whole list of 3 million possible barcodes from cellranger. - filtered matrices in bundle (only for Velocytoon10Xfrombundled workflow): A collection of filtered matrices as bundled (like the one which comes from the fastq-to-matrix-10x workflows): A collection with as many items as samples. For each sample, the item is a list with 3 datasets (barcodes, genes, matrix). The workflow will then extract the items which have the 'barcodes' identifier. - gtf file: A file with annotations where exons are and how they are grouped into genes. Processing - If you provided matrices, the first step is to extract barcodes. - For both cases velocyto cli is run to get a loom file per sample with spliced and unspliced counts.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/scRNAseq/velocyto/Velocyto-on10X-filtered-barcodes.ga"
  },
  {
    "workflow_id": "iwc_brew3r",
    "category": "transcriptomics",
    "workflow_repository": "brew3r",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie_merge/2.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/brew3r_r/brew3r_r/1.0.2+galaxy1"
    ],
    "readme_cleaned": "BREW3R ![BREW3R logo](https://raw.githubusercontent.com/lldelisle/BREW3R/main/images/logo.png) BREW3R stands for Bulk RNA-seq Evidence-based Workflow for 3' UTR Reannotation. This workflow enables extending an existing gtf downloaded on a public website, like Ensembl, Genecode or UCSC, using de novo gene annotation with StringTie on full length bulk RNA-seq. BREW3R highly relies on a R package called BREW3R.r available on [bioconductor](https://bioconductor.org/packages/release/bioc/html/BREW3R.r.html). Input datasets - The workflow requires an input gtf file which will be extended. - As well as a collection of BAM files. Input values - strandedness: Must be one of stranded - forward, stranded - reverse and unstranded. For stranded libraries, reverse means that the read is complementary to the coding sequence, forward means that the read is in the same orientation as the coding sequence. - minimum coverage: Minimum reads per bp coverage to consider for assembly in each de novo assembly (for each BAM file). Default: 10 - minimum FPKM for merge: Minimum FPKM value for a transcript to be included into the merged de novo assembly. Processing - StringTie is called once per input BAM file to compute de novo assembly. - StringTie is called to merge all outputs of previous steps. - BREW3R.r is run with the default parameters on the input gtf to extend and the output of StringTie. If the library was unstranded all merged transcripts without orientation that overlaps exons of both strands are not used for the extension.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/transcriptomics/brew3r/BREW3R.ga"
  },
  {
    "workflow_id": "iwc_goseq",
    "category": "transcriptomics",
    "workflow_repository": "goseq",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/goseq/goseq/1.50.0+galaxy0",
      "join1"
    ],
    "readme_cleaned": "Gene Ontology and KEGG Pathway Enrichment Analysis Inputs dataset The workflow need the following inputs: - The DEG file: - A tabular file with first column the gene symbol and second column a boolean value whether the gene is a differentially expressed gene or not. - The gene length file: - A tabular file with first column the gene symbol and second column the gene length of the genes. - You can create this file with [Gene length and GC content](https://usegalaxy.eu/?toolid=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Flengthandgccontent%2Flengthandgccontent%2F0.1.2&version=latest) tool. You will need a GTF file as input. - If you are using [featureCounts](https://usegalaxy.eu/?toolid=toolshed.g2.bx.psu.edu%2Frepos%2Fiuc%2Ffeaturecounts%2Ffeaturecounts%2F2.0.6%2Bgalaxy0&version=latest) you can set Create gene-length file to yes and get gene length as separate output. - The KEGG file: - A tabular file with first column the Pathway ID and second column the Pathway name like: - ID Name 01100 Metabolic pathways - mmus 01200 Carbon metabolism - mmus - You can get this information from the KEGG database. For example: - mouse: https://rest.kegg.jp/list/pathway/mmu - human: https://rest.kegg.jp/list/pathway/hsa - Genome: Select one of the available genomes - Gene ID format: Select the format of your input genes (Ensembl, Entrez, or Symbol) Processing - The workflow will do a simple enrichment analysis with taking into account the gene length - The output will be 3 files GO table, Top ontology plot and DE genes in each category for Cellular Component, Biological Processes, and Molecular Function ontologies and KEGG table and DE genes in each KEGG Pathways Contribution @nilchia wrote the workflow and the tests.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/transcriptomics/goseq/goseq-go-kegg-enrichment-analsis.ga"
  },
  {
    "workflow_id": "iwc_rnaseq-de",
    "category": "transcriptomics",
    "workflow_repository": "rnaseq-de",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/deseq2/deseq2/2.11.40.8+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/deg_annotate/deg_annotate/1.1.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.3+galaxy1",
      "param_value_from_file",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/iuc/volcanoplot/volcanoplot/0.0.7",
      "join1",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/iuc/ggplot2_heatmap2/ggplot2_heatmap2/3.2.0+galaxy1"
    ],
    "readme_cleaned": "RNA-Seq Differential Expression Analysis with Visualization This workflow works only with an experimental setup containing exactly 2 conditions with at least 2 replicates per condition. Inputs dataset - Counts from changed condition: Counts from experimental condition or changed condition. For eg. counts from treatment or knockdown samples. - Counts from reference condition: Counts from reference condition or base condition. For eg. counts from untreated or wildtype samples. - Gene Annotaton: The same GTF file used for mapping and quantification. It is used to annotate the DESeq2 results table. Ideally, the GTF file should contain geneid, genebiotype and genename attributes. Inputs values - Count files have header: Indicate whether your input count files have a header line. Usually, count files generated from featureCounts tool have a header line whereas count files from RNA-STAR do not have. - Adjusted p-value threshold: All the genes with an adjusted p-value less than this value are considered as differentially expressed. With a value of 0.05, expect 5% of false positives in the differentially expressed genes list. If empty, a default value of 0.05 is used. - log2 fold change threshold: All the genes with an absolute fold change (regarless of up or down regulation) more than this value are selected. A log2 FC of 3 equals to an absolute fold change of 8 (2³). If empty, a default value of 1.0 is used. Processing - The workflow uses DESeq2 for performing differential expression analysis. In addition to the results table, it also produces normalized counts table. - The results table is annotated with gene positions, biotypes, gene symbols. - The annotated results table is further filtered with the input adjusted p-value and log2 fold change thresholds. - A valcano plot is generated with top 10 significantly differentially expressed genes. - A heatmap of log trasformed normalized counts and another heatmap of Z-scores is generated.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/transcriptomics/rnaseq-de/rnaseq-de-filtering-plotting.ga"
  },
  {
    "workflow_id": "iwc_rnaseq-pe",
    "category": "transcriptomics",
    "workflow_repository": "rnaseq-pe",
    "tool_names": [
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.11a+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/featurecounts/featurecounts/2.0.8+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/cufflinks/cufflinks/2.2.1.4",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0"
    ],
    "readme_cleaned": "RNA-Seq Analysis: Paired-End Read Processing and Quantification Inputs dataset - Collection paired FASTQ files: The workflow needs a list of dataset pairs of fastqsanger. - GTF file of annotation: A gtf file with genes annotation. - GTF with regions to exclude from FPKM normalization with Cufflinks: Optional, but recommended. A gtf file with regions to exclude from normalization in Cufflinks. - For instance a gtf that masks chrM for the mm10 genome: chrM chrMgene exon 0 16299 . + . geneid \"chrMgeneplus\"; transcriptid \"chrMtxplus\"; exonid \"chrMexplus\"; chrM chrMgene exon 0 16299 . - . geneid \"chrMgeneminus\"; transcriptid \"chrMtxminus\"; exonid \"chrMexminus\"; Inputs values - Forward and Reverse adapter (optional): By default, fastp will try to overlap both reads and will only use these sequences if R1/R2 are found not overlapped. Their sequences depends on the library preparation. Usually classical Illumina RNA libraries is Truseq and ISML (relatively new Illumina library) is Nextera. - Generate additional QC reports: whether to compute additional QC: FastQC, Picard, Read distribution on genomic features, gene body coverage, reads per chromosomes. - Reference genome: this field will be adapted to the genomes available for STAR. - Strandedness: For stranded RNA, reverse means that the first read in a pair is complementary to the coding sequence, forward means that the first read in a pair is in the same orientation as the coding sequence. This will only count alignments that are compatible with your library preparation strategy. This is also used for the stranded coverage and for FPKM computation with cufflinks/StringTie. - Use featureCounts for generating count tables: Whether to use count tables from featureCounts instead of from STAR. - Compute Cufflinks FPKM: Whether you want to get FPKM with Cufflinks (pretty long). - Compute StringTie FPKM: Whether you want to get FPKM/TPM etc... with StringTie. Processing - The workflow will remove adapters and low quality bases and filter out any read smaller than 15bp. - The filtered reads are mapped with STAR with ENCODE parameters (for long RNA-seq but I use it for short also). STAR is also used to count reads per gene and generate strand-specific normalized coverage (on uniquely mapped reads). - Optionally featureCounts is used to generate count files when this option enabled. - Optionally FastQC, Picard, readdistribution, geneBodycoverage, samtools idxstats, Picard are run to get additional QC. - A multiQC is run to have an overview of the QC. This can also be used to get the strandedness. - FPKM values for genes and transcripts are computed with cufflinks using correction for multi-mapped reads (this step is optionnal). - FPKM/TPM values for genes are computed with StringTie (this step is optional). - The BAM is filtered to keep only uniquely mapped reads (tag NH:i:1). - Unstranded coverage is computed with bedtools and normalized to the number of million uniquely mapped reads. - The three coverage files are converted to bigwig. Warning - The coverage stranded output depends on the strandedness of the library: - If you have an unstranded library, stranded coverages are useless - If you have a forward stranded library, the label matches the orientation of the first read in pairs. - If you have a reverse stranded library, the label matches the orientation of the second read in pairs.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/transcriptomics/rnaseq-pe/rnaseq-pe.ga"
  },
  {
    "workflow_id": "iwc_rnaseq-sr",
    "category": "transcriptomics",
    "workflow_repository": "rnaseq-sr",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.0+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/rgrnastar/rna_star/2.7.11a+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/featurecounts/featurecounts/2.0.8+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/stringtie/stringtie/2.2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/cufflinks/cufflinks/2.2.1.4",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy0"
    ],
    "readme_cleaned": "RNA-Seq Analysis: Single-End Read Processing and Quantification Inputs dataset - Collection of FASTQ files: The workflow needs a list of datasets of fastqsanger. - GTF file of annotation: A gtf file with genes annotation. - GTF with regions to exclude from FPKM normalization with Cufflinks: Optional, but recommended. A gtf file with regions to exclude from normalization in Cufflinks. - For instance a gtf that masks chrM for the mm10 genome: chrM chrMgene exon 0 16299 . + . geneid \"chrMgeneplus\"; transcriptid \"chrMtxplus\"; exonid \"chrMexplus\"; chrM chrMgene exon 0 16299 . - . geneid \"chrMgeneminus\"; transcriptid \"chrMtxminus\"; exonid \"chrMexminus\"; Inputs values - Forward adapter (optional): If not provided, fastp will try to guess the adapter sequence from the data. Its sequences depends on the library preparation. Usually classical Illumina RNA libraries are Truseq and ISML (relatively new Illumina library) is Nextera. If you don't know, use FastQC to determine if it is Truseq or Nextera. If the read length is relatively short (50bp), there is probably no adapter so it will not impact your results. - Generate additional QC reports: whether to compute additional QC: FastQC, Picard, Read distribution on genomic features, gene body coverage, reads per chromosomes. - Reference genome: this field will be adapted to the genomes available for STAR. - Strandedness: For stranded RNA, reverse means that the read is complementary to the coding sequence, forward means that the read is in the same orientation as the coding sequence. This will only count alignments that are compatible with your library preparation strategy. This is also used for the stranded coverage and for FPKM computation with cufflinks/StringTie. - Use featureCounts for generating count tables: Whether to use count tables from featureCounts instead of from STAR. - Compute Cufflinks FPKM: Whether you want to get FPKM with Cufflinks (pretty long). - Compute StringTie FPKM: Whether you want to get FPKM/TPM etc... with StringTie. Processing - The workflow will remove adapters and low quality bases and filter out any read smaller than 15bp. - The filtered reads are mapped with STAR with ENCODE parameters (for long RNA-seq but I use it for short also). STAR is also used to count reads per gene and generate strand-specific normalized coverage (on uniquely mapped reads). - A multiQC is run to have an overview of the QC. This can also be used to get the strandedness. - FPKM values for genes and transcripts are computed with cufflinks using correction for multi-mapped reads (this step is optionnal). - FPKM/TPM values for genes are computed with StringTie (this step is optional). - The BAM is filtered to keep only uniquely mapped reads (tag NH:i:1). - Unstranded coverage is computed with bedtools and normalized to the number of million uniquely mapped reads. - The three coverage files are converted to bigwig. Warning - The coverage stranded output depends on the strandedness of the library: - If you have an unstranded library, stranded coverages are useless - If you have a forward stranded library, the label matches the orientation of reads. - If you have a reverse stranded library, forward should correspond to genes on the forward strand and uses the reads mapped on the reverse strand. reverse should correspond to genes on the reverse strand and uses the reads mapped on the forward strand. Contribution Version 0.1 @lldelisle wrote the workflow and the tests. @nagoue updated the tools, made it work in usegalaxy.org, fixed some best practices. Version 1.0 @pavanvidem added the new features (featurecount + additional QC) and found a smaller test dataset.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/transcriptomics/rnaseq-sr/rnaseq-sr.ga"
  },
  {
    "workflow_id": "iwc_generic-variant-calling-wgs-pe",
    "category": "variant-calling",
    "workflow_repository": "generic-variant-calling-wgs-pe",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.23.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_build_gb/4.3+T.galaxy4",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.13+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/2.18.2.2",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.2+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_indelqual/lofreq_indelqual/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_filter/lofreq_filter/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff/4.3+T.galaxy1"
    ],
    "readme_cleaned": "Generic variation analysis on WGS PE data ------------------------------------------- This workflows performs paired end read mapping with bwa-mem followed by sensitive variant calling across a wide range of AFs with lofreq and variant annotation with snpEff. The reference genome can be provided as a GenBank file.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/variant-calling/generic-variant-calling-wgs-pe/Generic-variation-analysis-on-WGS-PE-data.ga"
  },
  {
    "workflow_id": "iwc_haploid-variant-calling-wgs-pe",
    "category": "variant-calling",
    "workflow_repository": "haploid-variant-calling-wgs-pe",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.23.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_build_gb/4.3+T.galaxy6",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.17.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.13+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.2+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/devteam/picard/picard_MarkDuplicates/2.18.2.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.11+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_call/lofreq_call/2.1.5+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_awk_tool/9.3+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff/4.3+T.galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0"
    ],
    "readme_cleaned": "Haploid variant calling for whole genome sequencing paired end data This workflow uses Illumina or Element read data to discover variants (short nucleotide polymorphisms, SNPs, and small indels) in haploid genomes with multiple genomic sequences (contigs, scaffolds, or chromosomes). Inputs dataset - The workflow needs a list of paired end fastq files - A GTF containtaing the Gene annotation for the selected haploid genome - A fasta file for the haploid genome to call variants against Outputs - Tab-delimited summary of annotated variants - Report summarizing the quality of input data and mapping results Processing - The workflow will remove adapters using fastp - The filtered reads are aligned with bwa-mem. - Only properly aligned mate pairs are retained, PCR duplicates are removed. - Alignments are re-aligned using lofreq viterbi and variants are called with lofreq call. - Variants are annotated with snpeff eff",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/variant-calling/haploid-variant-calling-wgs-pe/WGS-PE-variant-calling-in-haploid-system.ga"
  },
  {
    "workflow_id": "iwc_variation-reporting",
    "category": "variant-calling",
    "workflow_repository": "variation-reporting",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_filter/4.3+t.galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/column_maker/Add_a_column1/1.6",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.1.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/1.1.3",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "Filter1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/1.1.2",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sort_header_tool/1.1.1"
    ],
    "readme_cleaned": "Generic variation analysis reporting -------------------------------------- This workflow takes table of variants produced by any of the variant calling workflows in https://github.com/galaxyproject/iwc/tree/main/workflows/variant-calling and generates a list of variants by Samples and by Variant.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/variant-calling/variation-reporting/Generic-variation-analysis-reporting.ga"
  },
  {
    "workflow_id": "iwc_generic-non-segmented-viral-variant-calling",
    "category": "virology",
    "workflow_repository": "generic-non-segmented-viral-variant-calling",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/1.0.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff_build_gb/5.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/map_param_value/map_param_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/pick_value/pick_value/0.2.0",
      "toolshed.g2.bx.psu.edu/repos/iuc/calculate_numeric_param/calculate_numeric_param/0.1.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.19",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.7",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.21+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/lofreq_viterbi/lofreq_viterbi/2.1.5+galaxy0",
      "__FILTER_FAILED_DATASETS__",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.4.4+galaxy1",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.24.1+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_variants/ivar_variants/1.4.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_consensus/ivar_consensus/1.4.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpeff/snpEff/5.2+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/snpsift/snpSift_extractFields/4.3+t.galaxy0",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0"
    ],
    "readme_cleaned": "Variant calling and consensus sequence construction from paired-end sequencing data of simple non-segmented viruses This workflow performs variant calling and consensus sequence generation for batches of Illumina PE sequenced viruses with uncomplicated and stable genome structure (like e.g. Morbilliviruses). It can handle both ampliconic and non-ampliconic data. If a primer scheme is provided as optional input, this is taken to indicate ampliconic sequencing data and the scheme will be used as the basis for trimming of primer sequences from mapped reads and for the exclusion of reads mapping beyond amplicon boundaries. Without a primer scheme non-ampliconic data is assumed. It uses: - fastp for sequenced reads pre-processing, - bwa mem for mapping - samtools view to keep only reads mapped in a proper pair with a mapping quality > 20 - lofreq viterbi to realign mapped reads around indels - ivar trim for trimming of primer sequences from mapped reads and read filtering; gets skipped if no \"Primer scheme\" input is provided - ivar call and ivar consensus for parallel calling of variants and consensus sequence generation - SnpEff for annotating variant calls with their functional genomic effects on the features declared in the \"Reference annotation\" - SnipSift for extracting information from SnpEff annotated VCFs into tabular variant reports - fastp, samtools stats, Qualimap bamqc and MultiQC for reporting on quality control results Input datasets - Paired collection of sequencing data: a list of pairs of sequencing datasets, one fw-/rv-reads pair per sequenced isolate - Fasta reference genome: a suitable reference genome for your virus provided in FASTA format - Reference annotation: genome annotations of the reference sequence in GTF format. Please make sure it is matching your reference genome sequence! - Primer scheme (optional): If provided, indicates ampliconic data and will trigger primer trimming and removal of reads that extend beyond amplicon boundaries. Please make sure the scheme matches your reference sequence and that the format of the dataset is set to bed! Input parameters - Supporting read fraction to call variant: This sets the lower threshold for the fraction of variant-supporting reads needed at a site to call that variant. The value can be set to a floating point number between 0.05 (very sensitive calling) and 0.25 (more noise-resistant calling, the default). This setting also affects the frequency threshold used for consensus building, which will be set to 0.95 minus the configured value, i.e. with the default setting of 0.25, the most frequent bases at each site will be added up until their combined fraction reaches 0.95 - 0.25 = 0.7 and the IUPAC ambiguity code representing all contributing bases will be used in the consensus. With a setting of 0.05, bases will be added up until they reach a combined fraction of 0.95 - 0.05 = 0.9, i.e. IUPAC ambiguity codes will get incorporated with a higher chance. In summary, with a higher setting of this parameter the workflow will generate less (but more reliable) variant calls and a cleaner consensus with less ambiguity codes. Minimum quality score to consider base for variant calling: The workflow default for this parameter is 20 and it will be applied to both variant calling and consensus building. Outputs: - Processed mapped reads (filtered and realigned): the filtered and realigned mapped reads in BAM format that serve as input for variant calling and consensus building for non-ampliconic data - Processed mapped reads (filtered and realigned, primers trimmed): the filtered, realigned and primer-trimmed mapped reads in BAM format that serve as input for variant calling and consensus building for ampliconic data - SnpEff-annotated variants: a collection of SnpEff-annotated variant calls in vcf format, one dataset per sample in the batch - Combined variant report for all samples: a concatenated, flat tabular report of variants and their annotations from all samples - Per-sample consensus genomes: a collection of consensus sequences in Fasta format, one dataset per sample - Combined consensus genomes for all samples: the consensus sequences of all samples concatenated in multi-Fasta format - Quality control report: report of quality control results from fastp, samtools stats (on the results of mapping with bwa-mem) and Qualimap BamQC (on the filtered mapped reads) aggregated with MultiQC Known issues and limitations - Non-matching reference genome sequence, genome annotation and/or primer scheme It is critical for analysis results that the sequence assumed in the genome annotation and, if provided, in the primer scheme input is the same as the reference sequence. Violation of this condition can lead to failures of the workflow run at the SnpEff build or ivar trim (for ampliconic data) steps, but can also cause misannotations of variant effects, wrong primer trimming or inappropriate read elimination before variant calling and consensus building in seemingly successful workflow runs. - Failing pimer trimming for ampliconic data This is usually caused by one of the following issues with your primer scheme input: - The format of the primer scheme dataset in your history is not set to bed, but to e.g. interval. You either selected the wrong format when uploading the data, or you had Galaxy autodetect the format and it wasn't recognized as bed. Please change the dataset format manually in this case and rerun the WF. - The primer scheme might not be fully parseable by Galaxy's ivar trim wrapper. In particular, the tool applies the regular expression pattern: .(?P \\d+).(?P L(?:EFT)?|R(?:IGHT)?) to the primer names in column 4 of the primer scheme to deduce amplicon names and primer orientation. This means that it will be able to parse primer names like the following correctly: nCoV-20191LEFT (parsed as forward primer of amplicon 1), 4002outR (parsed as reverse primer of amplicon 2), QIAseq163-2LEFT (parsed as forward primer of amplicon 163) or 177e6ebb0LEFT0 (parsed as one of several alternative forward primers of amplicon 177e6ebb), but more exotic names might fail parsing. If parsing of the primer names is the issue, then you need to edit the names on column 4 of your primer scheme file to pass the regular expression pattern. - Failure to annotate all translation products in complex viral genes This generic workflow can currently, at the annotation step, not handle all ways in which your virus may produce several translation products from a single viral gene. Alternative translation products resulting from ribosomal slippage or polymerase stuttering will, for example, not be annotated in the SnpEff-annotated variants and the Combined variant report for all samples outputs of the workflow. For other translation products stemming from, for example, the use of alternative start codons or alternative ribosomal entry points, successful annotation will depend on the details of how those products are represented in your Reference annotation input. When running the workflow for the first time with a new Reference annotation, always check carefully the predicted effects on genes, transcripts and proteins in the SnpEff-annotated variants and the Combined variant report for all samples outputs to understand virus and annotation-file specific problems and limitations before basing your interpretation on those reports.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/virology/generic-non-segmented-viral-variant-calling/pe-illumina-simple-virus-calling-and-consensus.ga"
  },
  {
    "workflow_id": "iwc_influenza-isolates-consensus-and-subtyping",
    "category": "virology",
    "workflow_repository": "influenza-isolates-consensus-and-subtyping",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_text_file_with_recurring_lines/9.5+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/collection_element_identifiers/collection_element_identifiers/0.0.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/0.24.1+galaxy0",
      "wc_gnu",
      "__UNZIP_COLLECTION__",
      "param_value_from_file",
      "__DUPLICATE_FILE_TO_COLLECTION__",
      "__RELABEL_FROM_FILE__",
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/iuc/vapor/vapor/1.0.2+galaxy3",
      "__FLATTEN__",
      "__FILTER_FAILED_DATASETS__",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_find_and_replace/9.5+galaxy0",
      "Show beginning1",
      "toolshed.g2.bx.psu.edu/repos/nml/collapse_collections/collapse_dataset/5.1.0",
      "__MERGE_COLLECTION__",
      "Grep1",
      "__HARMONIZELISTS__",
      "Paste1",
      "toolshed.g2.bx.psu.edu/repos/iuc/seqtk/seqtk_subseq/1.4+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_easyjoin_tool/9.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.19",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.20+galaxy3",
      "toolshed.g2.bx.psu.edu/repos/iuc/bamtools_split_ref/bamtools_split_ref/2.5.2+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_consensus/ivar_consensus/1.4.4+galaxy0",
      "Filter1",
      "Cut1",
      "__FILTER_FROM_FILE__",
      "toolshed.g2.bx.psu.edu/repos/rnateam/mafft/rbc_mafft/7.526+galaxy1",
      "toolshed.g2.bx.psu.edu/repos/iuc/snipit/snipit/1.6+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/iqtree/iqtree/2.4.0+galaxy0"
    ],
    "readme_cleaned": "Subtyping and consensus sequence generation for batches of Influenza A isolates This workflow performs subtyping (with respect to the HA and NA genes) and consensus sequence generation for batches of Illumina PE sequenced Influenza isolates. It overcomes Influenza genome variability by compiling the best possible reference for read mapping from a collection of reference sequences for each viral genome segment. The workflow has been tested for Influenza A isolates only but should also work with other types if a suitable collection of per-segment reference sequences (see \"Input datasets\" below) is provided. It uses: - VAPOR for identifying, from a collection of reference sequences of each of the eight viral genome segments, the individual segment sequences that match the sequencing data for each sample most closely - fastp for sequenced reads pre-processing, - bwa mem for mapping the preprocessed reads to a reference compiled from the best-matching segment sequences identified by VAPOR - ivar consensus for generating consensus sequences from the mapped reads data - MAFFT for generating multiple sequence alignments from the consensus sequences of the samples in a batch - Snipit and IQ-Tree for visualizing differences and analyzing relationships between sequences from a batch The workflow provides a subtyping report, the consensus sequences arranged by gene segment or by sample, quality control at the level of sequenced reads and of mapping results, and batch-level visualization and phylogenetic insight. Input datasets - Sequenced paired-end data: a list of pairs of sequencing datasets, one fw-/rv-reads pair per sequenced isolate - References per segment collection: this must be provided as a list of FASTA datasets, one for each Influenza genome segment to analyze. Each of these datasets should contain all reference sequences of the corresponding segment that you wish to use in the analysis. NOTE 1: For subtyping to work correctly all FASTA sequence title lines in the input need to follow the scheme (note the use of both | and /): >[segment]|[influenza type]/[host]/[region]/[internal reference number]/[collection year]|subtype|[accession number] where [host] can be omitted for samples obtained from human patients. Examples: - >NS|A/California/07/2009|H1N1|NC026432.1 (for the NS segment sequence of a H1N1 sample obtained from a human patient in California in 2009) - >NA|A/chicken/Zimbabwe/AI4935/2017|H5N8|MF973227.1 (for the NA segment sequence of a H5N8 sample obtained from chicken in Zimbabwe in 2017) NOTE 2: Sequences containing the ambiguity symbol N will be ignored by the workflow entirely. NOTE 3: Datasets in this collection must not use colons (:) as part of their names. NOTE 4: A well-formatted collection of Influenza A reference sequences suitable for most analysis needs is linked from the \"Data resources\" section of the page: https://virology.usegalaxy.eu/published/page?id=a04ab8d6ecb698fa. Outputs: - successful VAPOR runs - closest references: a nested collection listing, for each sample and segment, the up to 500 best matching reference sequences according to VAPOR; inspect this collection if you are curious how many good matches to your data there were in the reference collection; useful for debugging, for example, if generated consensus sequences contain unresolved bases (Ns); missing segments or samples in this collection indicate that VAPOR failed to identify any matches for the respective item - Subtyping results: a table listing one sample and its detected subtype (with respect to the HA and NA segments) per row; missing subtype information for HA, NA or both segments is indicated by H?, N? and H?N?, respectively, in the corresponding sample line. - Hybrid reference genomes used for mapping: collection of compiled reference genomes (in FASTA format, one per sample) that served as input for bwa-mem; each reference genome consists of the genome segments from the reference collections that best matched the corresponding sample's sequencing data - fastp reports: QC and read trimming and filtering results from fastp - Final read mapping results: bwa-mem mapping results in BAM format post-processed with samtools view - QC reports for mapping results: a collection of reports of QC metrics for the \"Final read mapping results\" generated with Qualimap - Per-sample consensus sequences: a nested collection of consensus sequences organized first by sample, then by segment - Per-segment consensus sequences with samples combined: a collection of the same consensus sequences organized by segment; each collection element is a multi-sequence FASTA dataset with the segment-specific sequences of all samples - Multiple sequence alignments per segment: a collection of multiple sequence alignments generated with MAFFT from each of the \"Per-segment consensus sequences with samples combined\" above; generated only for segments for which at least two samples yielded a consensus sequence - Snipit plots per segment: a collection of SNP plots across samples generated with Snipit; one plot per segment; generated only for segments for which at least two samples yielded a consensus sequence; the first input sample will be used as the reference in the plot - IQ-Tree per-segment ML tree, IQ-Tree per-segment ML distance matrix and IQ-Tree per-segment report: collections of IQ-Tree ML trees, distance matrices and reports; each collection has one element per segment; generated only for segments for which at least three samples yielded a consensus sequence Related training material https://gxy.io/GTN:T00308 guides you through a simplified, manual analysis that still includes the key steps of this workflow.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/virology/influenza-isolates-consensus-and-subtyping/influenza-consensus-and-subtyping.ga"
  },
  {
    "workflow_id": "iwc_pox-virus-amplicon",
    "category": "virology",
    "workflow_repository": "pox-virus-amplicon",
    "tool_names": [
      "toolshed.g2.bx.psu.edu/repos/devteam/fasta_compute_length/fasta_compute_length/1.0.4",
      "Grep1",
      "toolshed.g2.bx.psu.edu/repos/iuc/collection_element_identifiers/collection_element_identifiers/0.0.2",
      "toolshed.g2.bx.psu.edu/repos/iuc/fastp/fastp/1.0.1+galaxy2",
      "Cut1",
      "toolshed.g2.bx.psu.edu/repos/iuc/datamash_ops/datamash_ops/1.9+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/bgruening/split_file_to_collection/split_file_to_collection/0.5.2",
      "__SORTLIST__",
      "param_value_from_file",
      "toolshed.g2.bx.psu.edu/repos/iuc/compose_text_param/compose_text_param/0.1.1",
      "toolshed.g2.bx.psu.edu/repos/devteam/emboss_5/EMBOSS: maskseq51/5.0.0",
      "toolshed.g2.bx.psu.edu/repos/devteam/bwa/bwa_mem/0.7.19",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_view/samtools_view/1.21+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/devteam/samtools_stats/samtools_stats/2.0.7",
      "toolshed.g2.bx.psu.edu/repos/iuc/multiqc/multiqc/1.27+galaxy3",
      "__ZIP_COLLECTION__",
      "__APPLY_RULES__",
      "toolshed.g2.bx.psu.edu/repos/iuc/samtools_merge/samtools_merge/1.21+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/qualimap_bamqc/qualimap_bamqc/2.3+galaxy0",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_trim/ivar_trim/1.4.4+galaxy1",
      "__FILTER_FAILED_DATASETS__",
      "toolshed.g2.bx.psu.edu/repos/iuc/ivar_consensus/ivar_consensus/1.4.4+galaxy0",
      "__FLATTEN__",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_sed_tool/9.5+galaxy2",
      "toolshed.g2.bx.psu.edu/repos/bgruening/text_processing/tp_cat/9.5+galaxy2"
    ],
    "readme_cleaned": "Pox Virus Illumina Amplicon Workflow for half-genomes sequencing data This workflow generates consensus sequences from Illumina PE-sequenced ARTIC data of pox virus samples. It requires that all samples have been sequenced in two halves in two separate sequencing runs, and utilizes this property to resolve the inverted terminal repeat (ITR) sequences of pox virus genomes. The workflow uses BWA-MEM for mapping the reads from each half-genome sequencing run to a correspondingly masked version of the reference genome, merges the resulting two read mappings, and uses iVar for primer trimming and consensus sequence generation. Conceptually, this workflow builds on https://github.com/iwc-workflows/sars-cov-2-pe-illumina-artic-ivar-analysis and adds the logic for the split genome mapping and merging of the results.",
    "raw_download_url": "https://raw.githubusercontent.com/galaxyproject/iwc/main/workflows/virology/pox-virus-amplicon/pox-virus-half-genome.ga"
  }
]